{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer \n",
    "import numpy as np\n",
    "import json\n",
    "import gradio as gr\n",
    "\n",
    "modele = SentenceTransformer('all-mpnet-base-v2') #Chargement d'un modèle SBERT\n",
    "\n",
    "#def Sauvegarde_JSON (Fichier):\n",
    "    #\"\"\"Sauvegarde les embeddings sous format JSON \"\"\"\n",
    "    #with open ('Projets/Mini/data.json','w') as json_file:\n",
    "        #json.dump(Charger_Documents(Fichier),json_file,indent= 4) # stocke les embeddings des documents dans un fichier JSON\n",
    "\n",
    "def Charger_Documents(Fichier):\n",
    "    \"\"\"Charge les documents/Articles du Fichier et renvoie un dictrionnaire de format {Article (article) : Embeddings (valeur)\"\"\"\n",
    "    global Documents_Embeddings\n",
    "    Documents_Embeddings = [] \n",
    "    \n",
    "    #Intègre chaque document dans une liste\n",
    "    liste_Documents = []\n",
    "    with open(Fichier.name, 'r') as Fichier:\n",
    "        for Document in Fichier:\n",
    "            liste_Documents.append(Document.strip())\n",
    "\n",
    "    #Créer les embeddings pour tous les documents en une seule fois\n",
    "    embeddings = modele.encode(liste_Documents)\n",
    "    for i, (Document, embedding) in enumerate(zip(liste_Documents, embeddings), start=1):\n",
    "        #Création d'un tuple (i, (articles, embeddigs)) et le parcours pour créer un dictionnaire pour chaque article\n",
    "        Documents_Embeddings.append({\n",
    "        \"id\": i,\n",
    "        \"texte\": Document,\n",
    "        \"embedding\": embedding.tolist() \n",
    "    })\n",
    "    \n",
    "    #\"Sauvegarde les embeddings sous format JSON\n",
    "    with open ('Projets/Mini/data.json','w') as json_file:\n",
    "        json.dump(Documents_Embeddings,json_file,indent= 4) # stocke les embeddings des documents dans un fichier JSON\n",
    "\n",
    "    return \"Documents chargés\"\n",
    "\n",
    "#Charger_Documents('/Users/lnberroug/Documents/LDDBI-L2/Info/IN304/Projet/Projets/Mini/lois.txt')\n",
    "#Charger_Documents('Projets/Mini/lois.txt')\n",
    "\n",
    "\n",
    "def Encoder_Requete(requete):\n",
    "    \"\"\"Encode la requête saisie par l'utilisateur\"\"\"\n",
    "    embedding_requete = modele.encode(requete) # convertie la requête en un vecteur \n",
    "    return embedding_requete\n",
    "\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    \"\"\"Calcul de la similarité cosinus\"\"\" # source : https://medium.com/@santannalouis208/la-similarité-cosinus-en-ia-nlp-d554d3b14efa\n",
    "    #Produit scalaire des vecteurs\n",
    "    scalar_product = np.dot(vector1, vector2)\n",
    "    #Norme euclidienne des vecteurs\n",
    "    norm_vector1 = np.linalg.norm(vector1)\n",
    "    norm_vector2 = np.linalg.norm(vector2)\n",
    "    #Expression analytique du cosinus dans un espace euclidien\n",
    "    cosine = scalar_product / (norm_vector1 * norm_vector2)\n",
    "    return cosine\n",
    "\n",
    "def Similarite_Requete_Document(requete_utilisateur):\n",
    "    \"\"\"Calculez la similarité cosinus entre le vecteur de la requête et les vecteurs des documents pour identifier les documents les plus proches\"\"\"\n",
    "    if not Documents_Embeddings:\n",
    "        return \"Aucun document chargé. Veuillez d'abord charger un fichier.\"\n",
    "    \n",
    "    vecteur_requete = Encoder_Requete(requete_utilisateur)\n",
    "    similarities = [\n",
    "        {\n",
    "            'texte' : document['texte'],\n",
    "            'score' : cosine_similarity(document['embedding'], vecteur_requete)\n",
    "        }\n",
    "        for document in Documents_Embeddings\n",
    "    ]\n",
    "\n",
    "    score_trie = sorted(similarities, key = lambda x: x['score'], reverse = True)\n",
    "    Top_3 = score_trie[:3]\n",
    "\n",
    "    result = \"\\n\".join([f\"{doc['texte']}\\nScore de similarité: {doc['score']}\" for i, doc in enumerate(Top_3, 1)])\n",
    "    return result\n",
    "\n",
    "\n",
    "#INTERFACE GRADIO\n",
    "\n",
    "def Charger_Fichier_Interface(Fichier):\n",
    "    result = Charger_Documents(Fichier)  \n",
    "    return result\n",
    "\n",
    "def Recherche_Similarite_Interface(requete):\n",
    "    result = Similarite_Requete_Document(requete)  \n",
    "    return result\n",
    "\n",
    "#l'onglet  pour charger des documents \n",
    "interface_charger = gr.Interface(\n",
    "    fn=Charger_Fichier_Interface,\n",
    "    inputs=gr.File(label=\"Charger le fichier texte\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Charger et Sauvegarder des Documents\",\n",
    "    description=\"Cliquez sur le bouton pour charger et sauvegarder les documents avec leurs embeddings sous forme de fichier JSON.\"\n",
    ")\n",
    "\n",
    "#l'onglet pour la recherche\n",
    "interface_recherche = gr.Interface(\n",
    "    fn=Recherche_Similarite_Interface,\n",
    "    inputs=gr.Textbox(label=\"Entrez votre requête\", placeholder=\"Saisir une requête ici...\", lines=2),\n",
    "    outputs=\"text\",\n",
    "    title=\"Recherche d'Articles\",\n",
    "    description=\"Entrez votre requête et obtenez les articles les plus pertinents.\"\n",
    ")\n",
    "\n",
    "#lancer l'interface\n",
    "demo = gr.TabbedInterface([interface_recherche, interface_charger], [\"Recherche\", \"Charger Document\"])\n",
    "demo.launch()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer \n",
    "import numpy as np\n",
    "import json\n",
    "import gradio as gr\n",
    "\n",
    "modele = SentenceTransformer('all-mpnet-base-v2') #Chargement d'un modèle SBERT\n",
    "\n",
    "def Sauvegarde_JSON (Fichier):\n",
    "    \"\"\"Sauvegarde les embeddings sous format JSON \"\"\"\n",
    "    with open ('/Users/lnberroug/Documents/LDDBI-L2/Info/IN304/Projet/Projets.json','w') as json_file:\n",
    "        json.dump(Fichier,json_file,indent= 4) # stocke les embeddings des documents dans un fichier JSON\n",
    "    return Fichier\n",
    "\n",
    "def Charger_Documents(Fichier):\n",
    "    \"\"\"Charge les documents/Articles du Fichier et renvoie un dictrionnaire de format {Article (article) : Embeddings (valeur)\"\"\"\n",
    "    global Documents_Embeddings\n",
    "    Documents_Embeddings = [] \n",
    "    \n",
    "    #Intègre chaque document dans une liste\n",
    "    liste_Documents = []\n",
    "    with open(Fichier.name, 'r') as Fichier:\n",
    "        for Document in Fichier:\n",
    "            liste_Documents.append(Document.strip())\n",
    "\n",
    "    #Créer les embeddings pour tous les documents en une seule fois\n",
    "    embeddings = modele.encode(liste_Documents)\n",
    "    for i, (Document, embedding) in enumerate(zip(liste_Documents, embeddings), start=1):\n",
    "        #Création d'un tuple (i, (articles, embeddigs)) et le parcours pour créer un dictionnaire pour chaque article\n",
    "        Documents_Embeddings.append({\n",
    "        \"id\": i,\n",
    "        \"texte\": Document,\n",
    "        \"embedding\": embedding.tolist() \n",
    "    })\n",
    "        \n",
    "    Sauvegarde_JSON (Documents_Embeddings)\n",
    "    return \"Documents chargés\"\n",
    "\n",
    "#Charger_Documents('/Users/lnberroug/Documents/LDDBI-L2/Info/IN304/Projet/Projets/Mini/lois.txt')\n",
    "#Charger_Documents('Projets/Mini/lois.txt')\n",
    "\n",
    "\n",
    "def Encoder_Requete(requete):\n",
    "    \"\"\"Encode la requête saisie par l'utilisateur\"\"\"\n",
    "    embedding_requete = modele.encode(requete) # convertie la requête en un vecteur \n",
    "    return embedding_requete\n",
    "\n",
    "\n",
    "\n",
    "def Similarite_Requete_Document(requete_utilisateur):\n",
    "    \"\"\"Calculez la similarité cosinus entre le vecteur de la requête et les vecteurs des documents pour identifier les documents les plus proches\"\"\"\n",
    "    if not Documents_Embeddings:\n",
    "        return \"Aucun document chargé. Veuillez d'abord charger un fichier.\"\n",
    "    \n",
    "    vecteur_requete = Encoder_Requete(requete_utilisateur)\n",
    "    \n",
    "\n",
    "    # convertir tous les embeddings des documents en une matrice numpy : permet de calculer la similarité cosinus en une seule opération\n",
    "\n",
    "    embeddings_documents = np.array([doc['embedding'] for doc in Documents_Embeddings]) \n",
    "    \n",
    "    #normalisation des vecteurs\n",
    "    \n",
    "    norm_embeddings_documents = embeddings_documents / np.linalg.norm(embeddings_documents)\n",
    "    norm_vecteur_requete = vecteur_requete/np.linalg.norm(vecteur_requete)\n",
    "    \n",
    "    similarites = np.dot(norm_embeddings_documents,norm_vecteur_requete)\n",
    "    \n",
    "\n",
    "    similarities = [\n",
    "        {\n",
    "            'texte' : document['texte'],\n",
    "            'score' : similarites[i]\n",
    "        }\n",
    "        for i, document in enumerate(Documents_Embeddings) \n",
    "\n",
    "        ]\n",
    "    \n",
    "    score_trie = sorted(similarities, key = lambda x: x['score'], reverse = True)\n",
    "    Top_3 = score_trie[:3]\n",
    "\n",
    "    result = \"\\n\".join([f\"{doc['texte']}\\nScore de similarité: {doc['score']}\" for i, doc in enumerate(Top_3, 1)])\n",
    "    return result\n",
    "\n",
    "\n",
    "#INTERFACE GRADIO\n",
    "\n",
    "def Charger_Fichier_Interface(Fichier):\n",
    "    result = Charger_Documents(Fichier)  \n",
    "    return result\n",
    "\n",
    "def Recherche_Similarite_Interface(requete):\n",
    "    result = Similarite_Requete_Document(requete)  \n",
    "    return result\n",
    "\n",
    "#l'onglet  pour charger des documents \n",
    "interface_charger = gr.Interface(\n",
    "    fn=Charger_Fichier_Interface,\n",
    "    inputs=gr.File(label=\"Charger le fichier texte\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Charger et Sauvegarder des Documents\",\n",
    "    description=\"Cliquez sur le bouton pour charger et sauvegarder les documents avec leurs embeddings sous forme de fichier JSON.\"\n",
    ")\n",
    "\n",
    "#l'onglet pour la recherche \n",
    "interface_recherche = gr.Interface(\n",
    "    fn=Recherche_Similarite_Interface,\n",
    "    inputs=gr.Textbox(label=\"Entrez votre requête\", placeholder=\"Saisir une requête ici...\", lines=2),\n",
    "    outputs=\"text\",\n",
    "    title=\"Recherche d'Articles\",\n",
    "    description=\"Entrez votre requête et obtenez les articles les plus pertinents.\"\n",
    ")\n",
    "\n",
    "#lancer l'interface\n",
    "demo = gr.TabbedInterface([interface_recherche, interface_charger], [\"Recherche\", \"Charger Document\"])\n",
    "demo.launch()\n",
    "\n",
    "# hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer \n",
    "import numpy as np\n",
    "import json\n",
    "import gradio as gr\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "modele = SentenceTransformer('all-mpnet-base-v2') #Chargement d'un modèle SBERT\n",
    "\n",
    "def Sauvegarde_JSON (Fichier):\n",
    "    \"\"\"Sauvegarde les embeddings sous format JSON \"\"\"\n",
    "    global chemin_json  \n",
    "    repertoire_temporaire = tempfile.mkdtemp()\n",
    "    chemin_json = os.path.join(repertoire_temporaire, 'data.json') \n",
    "\n",
    "\n",
    "    with open (chemin_json,'w') as json_file:\n",
    "        json.dump(Fichier,json_file,indent= 4) # stocke les embeddings des documents dans un fichier JSON\n",
    "    return chemin_json\n",
    "\n",
    "def Charger_Documents(Fichier):\n",
    "    \"\"\"Charge les documents/Articles du Fichier et renvoie un dictrionnaire de format {Article (article) : Embeddings (valeur)\"\"\"\n",
    "    global Documents_Embeddings\n",
    "    Documents_Embeddings = [] \n",
    "    \n",
    "    #Intègre chaque document dans une liste\n",
    "    liste_Documents = []\n",
    "    with open(Fichier.name, 'r') as Fichier:\n",
    "        for Document in Fichier:\n",
    "            liste_Documents.append(Document.strip())\n",
    "\n",
    "    #Créer les embeddings pour tous les documents en une seule fois\n",
    "    embeddings = modele.encode(liste_Documents)\n",
    "    for i, (Document, embedding) in enumerate(zip(liste_Documents, embeddings), start=1):\n",
    "        #Création d'un tuple (i, (articles, embeddigs)) et le parcours pour créer un dictionnaire pour chaque article\n",
    "        Documents_Embeddings.append({\n",
    "        \"id\": i,\n",
    "        \"texte\": Document,\n",
    "        \"embedding\": embedding.tolist() \n",
    "    })\n",
    "    chemin_fichier_json = Sauvegarde_JSON(Documents_Embeddings)\n",
    "\n",
    "    return  chemin_fichier_json\n",
    "\n",
    "\n",
    "\n",
    "def Encoder_Requete(requete):\n",
    "    \"\"\"Encode la requête saisie par l'utilisateur\"\"\"\n",
    "    embedding_requete = modele.encode(requete) # convertie la requête en un vecteur \n",
    "    return embedding_requete\n",
    "\n",
    "\n",
    "\n",
    "def Similarite_Requete_Document(requete_utilisateur):\n",
    "    \"\"\"Calculez la similarité cosinus entre le vecteur de la requête et les vecteurs des documents pour identifier les documents les plus proches\"\"\"\n",
    "    if not Documents_Embeddings:\n",
    "        return \"Aucun document chargé. Chargez un fichier.\"\n",
    "    \n",
    "    vecteur_requete = Encoder_Requete(requete_utilisateur)\n",
    "\n",
    "    # convertir tous les embeddings des documents en une matrice numpy : permet de calculer la similarité cosinus en une seule opération\n",
    "\n",
    "    embeddings_documents = np.array([doc['embedding'] for doc in Documents_Embeddings]) \n",
    "    \n",
    "    #normalisation des vecteurs\n",
    "    \n",
    "    norm_embeddings_documents = embeddings_documents / np.linalg.norm(embeddings_documents)\n",
    "    norm_vecteur_requete = vecteur_requete/np.linalg.norm(vecteur_requete)\n",
    "    \n",
    "    similarites = np.dot(norm_embeddings_documents,norm_vecteur_requete)\n",
    "    \n",
    "\n",
    "    similarities = [\n",
    "        {\n",
    "            'texte' : document['texte'],\n",
    "            'score' : similarites[i]\n",
    "        }\n",
    "        for i, document in enumerate(Documents_Embeddings) \n",
    "\n",
    "        ]\n",
    "    \n",
    "    score_trie = sorted(similarities, key = lambda x: x['score'], reverse = True)\n",
    "    Top_3 = score_trie[:3]\n",
    "\n",
    "    result = \"\\n\".join([f\"{doc['texte']}\\nScore de similarité: {doc['score']}\" for i, doc in enumerate(Top_3, 1)])\n",
    "    return result\n",
    "\n",
    "\n",
    "#INTERFACE GRADIO\n",
    "\n",
    "def Charger_Fichier_Interface(Fichier):\n",
    "    chemin_fichier_json = Charger_Documents(Fichier)  \n",
    "    return chemin_fichier_json\n",
    "\n",
    "def Recherche_Similarite_Interface(requete):\n",
    "    result = Similarite_Requete_Document(requete)  \n",
    "    return result\n",
    "\n",
    "def Telecharger_Json():\n",
    "    global chemin_json\n",
    "    if chemin_json:\n",
    "        return gr.File(value=chemin_json) \n",
    "    return None  \n",
    "\n",
    "#l'onglet  pour charger des documents \n",
    "interface_charger = gr.Interface(\n",
    "    fn=Charger_Fichier_Interface,\n",
    "    inputs=gr.File(label=\"Charger le fichier texte\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Charger et Sauvegarder des Documents\",\n",
    "    description=\"Cliquez sur le bouton pour charger et sauvegarder les documents avec leurs embeddings sous forme de fichier JSON.\"\n",
    ")\n",
    "\n",
    "#l'onglet pour la recherche \n",
    "interface_recherche = gr.Interface(\n",
    "    fn=Recherche_Similarite_Interface,\n",
    "    inputs=gr.Textbox(label=\"Entrez votre requête\", placeholder=\"Saisir une requête ici...\", lines=2),\n",
    "    outputs=\"text\",\n",
    "    title=\"Recherche d'Articles\",\n",
    "    description=\"Entrez votre requête et obtenez les articles les plus pertinents.\"\n",
    ")\n",
    "\n",
    "#l'onglet pour le telechargement \n",
    "interface_telechargement = gr.Interface(\n",
    "    fn=Telecharger_Json,\n",
    "    inputs=None,  \n",
    "    outputs=gr.File(label=\"Téléchargez le fichier JSON\"),\n",
    "    title=\"Télécharger le fichier JSON\",\n",
    "    description=\"Téléchargez le fichier JSON contenant les embeddings des documents.\"\n",
    ")\n",
    "\n",
    "#lancer l'interface\n",
    "demo = gr.TabbedInterface([interface_recherche, interface_charger, interface_telechargement], [\"Recherche\", \"Charger Document\", \"Télécharger JSON\"])\n",
    "demo.launch()\n",
    "\n",
    "# hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
