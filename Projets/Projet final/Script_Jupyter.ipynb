{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etudiantes\n",
    "Lina Berroug\n",
    "et\n",
    "Jessica Karega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTATION DE MODULES\n",
    "\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lnberroug/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lnberroug/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/lnberroug/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Télécharger les stopwords en français si ce n'est pas déjà fait\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Ce notebook analyse les tweets d'un fichier JSON pour extraire des informations et les exploiter grâce à un dataFrame\n",
    "\n",
    "Nous utiliserons les bibliothèques suivantes :\n",
    "- `json` : pour manipuler les fichiers JSON\n",
    "- `pandas` : pour analyser les données tabulaires\n",
    "- `re` : pour extraire ou supprimer des expressions régulière d'un tweet\n",
    "- `TextBlob` : pour l'analyse de sentiments\n",
    "- `matplotlib` : pour visualiser sous forme de graphiques les données\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagramme des étapes de traitement\n",
    "Schéma qui montre les étapes suivies pour le projet :\n",
    "\n",
    "<img src=\"diagramme.png\" alt=\"Description de l'image\" style=\"width:40%; height:auto;\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARTIE : CREATION DU TWEET + SOUMISSION DU TWEET DANS LANDING ZONE\n",
    "\n",
    "#Création d'une Classe Tweet à laquelle seront associées les attributs suivant : auteurs, hashtag, mentions, sentiments, (topics???)\n",
    "\n",
    "class Tweet: #Plan/Modèle\n",
    "    def __init__(self, contenu): #Méthode (assisstant qui aide à donner à chaque tweet ses attributs)\n",
    "        self.contenu = contenu\n",
    "\n",
    "    def Nettoyage_Tweet(self):\n",
    "        \"\"\"Fonction qui supprime les caractères spéciaux d'un tweet\"\"\"\n",
    "        enlever_speciaux = re.sub(r'[^a-zA-Z0-9\\s@#]', '', self.contenu)\n",
    "        return enlever_speciaux\n",
    "\n",
    "class ZA:\n",
    "    def __init__(self, id, auteur, texte): #Méthode (assisstant qui aide à donner à chaque tweet ses attributs)\n",
    "        self.id = id\n",
    "        self.auteur = auteur\n",
    "        self.text = texte\n",
    "        self.hashtags = self.Extraction_Hashtags()\n",
    "        self.mentions = self.Extraction_Mentions()\n",
    "        self.sentiment = self.Extraction_Sentiments()\n",
    "\n",
    "    \n",
    "    def Extraction_Sentiments(self): \n",
    "        \"\"\"Extrait le sentiment du texte\"\"\"\n",
    "        blob = TextBlob(self.text)\n",
    "        polarite =  blob.sentiment.polarity\n",
    "        if polarite > 0:\n",
    "            sentiment = 'positif'\n",
    "        elif polarite < 0:\n",
    "            sentiment = 'négatif'\n",
    "        elif polarite == 0:\n",
    "            sentiment = 'neutre'\n",
    "        return sentiment\n",
    "    \n",
    "    def Extraction_Hashtags(self):  #A Faire \n",
    "        \"\"\"\"\"\"\n",
    "        hashtags = re.findall(r'#\\w+', self.text)\n",
    "        return hashtags\n",
    "    \n",
    "    def Extraction_Mentions(self):  #A Faire\n",
    "        \"\"\"Fonction qui extrait les mentions de personnes dans un tweet\"\"\"\n",
    "        mentions = re.findall(r'@[A-Za-z0-9_]+', self.text)\n",
    "\n",
    "        return mentions\n",
    "\n",
    "\n",
    "    def Id_Topic(tweet):\n",
    "\n",
    "        def traitement_texte(text):\n",
    "            text= text.lower() # mettre en minuscule\n",
    "            text= re.sub(r'[^a-z\\s]', '', text) \n",
    "            return text\n",
    "        def get_stopwords():\n",
    "            return stopwords.words('french')\n",
    "        stopwords_list = get_stopwords()\n",
    "        tweets= []\n",
    "        with open ('versailles_tweets_100.json','r',encoding='utf-8') as f:\n",
    "            donnee= json.load(f)\n",
    "        \n",
    "        tweets = [infos[\"text\"] for infos in donnee]\n",
    "        \n",
    "        texte_traite = [traitement_texte(tweet) for tweet in tweets]\n",
    "        \n",
    "        vecteur = TfidfVectorizer(stop_words= stopwords_list)  # Utilise les stopwords en français intégrés dans sklearn\n",
    "        a = vecteur.fit_transform(texte_traite)\n",
    "\n",
    "        # n_components=2 : Le modèle cherchera à identifier 2 sujets principaux dans les données textuelles.\n",
    "        lda = LatentDirichletAllocation(n_components=2, random_state=42)\n",
    "        lda.fit(a) # on utilise LDA sur la matrice X \n",
    "\n",
    "        topics = []\n",
    "        # Récupération du vocabulaire extrait par TfidfVectorizer\n",
    "        words = vecteur.get_feature_names_out()\n",
    "        for i, topic_weights in enumerate(lda.components_):\n",
    "            top_indices = topic_weights.argsort()[:-3:-1]  # Prendre les 2 mots les plus importants\n",
    "            top_words = [words[i] for i in top_indices]  # Utilisation des indices pour récupérer les mots correspondants dans la liste `words`.\n",
    "\n",
    "            topics.append(f\": {', '.join(top_words)}\")\n",
    "\n",
    "        tweet_traite = traitement_texte(tweet)\n",
    "        vecteur = TfidfVectorizer(stop_words= stopwords_list) # Utilise les stopwords en français intégrés dans sklearn\n",
    "        vecteur_tweet= vecteur.transform([tweet_traite]) \n",
    "        probabilities = lda.transform(vecteur_tweet)\n",
    "        topic = probabilities.argmax()  # Trouve l'indice du topic avec la probabilité la plus élevée\n",
    "\n",
    "        print(f\"Le tweet :\\n{tweet}\\nAppartient au Topic {topics[topic]}\")\n",
    "\n",
    "\n",
    "def Zone_Atterisssage (Fichier): # incomplet\n",
    "        \"\"\"Stockage du tweet dans le fichier json zone d'atterissage\"\"\"\n",
    "        ZA_liste=[]\n",
    "        \n",
    "        with open (Fichier,'r',encoding='utf-8') as f:\n",
    "            donnee= json.load(f)\n",
    "            \n",
    "        for infos_tweets in donnee:\n",
    "            id = infos_tweets.get(\"id\")\n",
    "            auteur = infos_tweets.get(\"author_id\")\n",
    "            contenu = infos_tweets.get(\"text\")\n",
    "\n",
    "            tweet = Tweet(contenu)\n",
    "            contenu_nettoye = tweet.Nettoyage_Tweet()\n",
    "    \n",
    "\n",
    "            tweet_analyse = ZA(id, auteur, contenu_nettoye)\n",
    "\n",
    "\n",
    "            dict_tweet = {\n",
    "                \"id\": tweet_analyse.id,\n",
    "                \"auteur\": tweet_analyse.auteur,\n",
    "                \"tweet\": contenu_nettoye,\n",
    "                \"texte\": tweet_analyse.text,\n",
    "                \"hashtags\" : tweet_analyse.hashtags,\n",
    "                \"mentions\" : tweet_analyse.mentions,\n",
    "                \"sentiment\" : tweet_analyse.sentiment\n",
    "            }\n",
    "\n",
    "\n",
    "            ZA_liste.append(dict_tweet)\n",
    "\n",
    "        \n",
    "        \n",
    "        with open('ZA.json','w', encoding='utf-8') as f:\n",
    "            json.dump(ZA_liste, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "# --> il faut charger ces données dans une date frame : df = pd.DataFrame(tweets)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATAFRAME\n",
    "def CreerDataFrame(fichier_json):\n",
    "    df  = pd.read_json(fichier_json)\n",
    "    return df\n",
    "\n",
    "#Analyses avancées \n",
    "\n",
    "def Top_K_element(df, colonne, k=3):\n",
    "    \"\"\"\n",
    "    Retourne les K éléments les plus fréquents dans une colonne donnée.\n",
    "    \"\"\"\n",
    "    top_k = df[colonne].explode().value_counts().head(k) #a méthode .explode() transforme chaque élément d'une liste dans une ligne distincte.\n",
    "    element= top_k.index.tolist()  # Liste des hashtags\n",
    "    counts = top_k.values.tolist()\n",
    "    return element,counts\n",
    "\n",
    "def publications_par_hashtag(df):\n",
    "    \"\"\"\n",
    "    Retourne le nombre de publications par hashtag.\n",
    "    \"\"\"\n",
    "    hashtags_explodes = df['hashtags'].explode()\n",
    "    hashtag_counts = hashtags_explodes.value_counts()\n",
    "\n",
    "    hashtags = hashtag_counts.index.tolist()  # Liste des hashtags\n",
    "    counts = hashtag_counts.values.tolist()   # Liste des nombres d'occurrences\n",
    "    return hashtags, counts\n",
    "\n",
    "def publications_par_utilisateurs(df):\n",
    "    \"\"\"\n",
    "    Retourne le nombre de publications par utilisateur.\n",
    "    \"\"\"\n",
    "    utilisateur_counts = df['auteur'].value_counts()\n",
    "    utilisateur = utilisateur_counts.index.tolist()  # Liste des utilisateurs\n",
    "    counts = utilisateur_counts.values.tolist()   # Liste des nombres d'occurrences\n",
    "    return utilisateur, counts\n",
    "\n",
    "def publications_par_topic(df):\n",
    "    \"\"\"\n",
    "    Retourne le nombre de publications par topics.\n",
    "    \"\"\"\n",
    "    topics_counts = df['topics'].value_counts()\n",
    "    topic = topics_counts.index.tolist()  # Liste des topics\n",
    "    counts = topics_counts.values.tolist()   # Liste des nombres d'occurrences\n",
    "    return topic , counts\n",
    "\n",
    "def tweets_par_utilisateur(df,utilisateur):\n",
    "    \"\"\"\n",
    "    Retourne le nombre de publications par topics.\n",
    "    \"\"\"\n",
    "    filtre = df[df['auteur'] == utilisateur]\n",
    "    tweets = filtre['contenu'].tolist() \n",
    "    utilisateurs = filtre ['auteur'].tolist() \n",
    "    return utilisateurs,tweets\n",
    "    \n",
    "def ensemble_de_tweets_mentionnant_utilisateur(df,utilisateur_mentionné):\n",
    "    \"\"\"\n",
    "    Retourne le nombre de publications par topics.\n",
    "    \"\"\"\n",
    "    filtre = df[df['mentions'] == utilisateur_mentionné]\n",
    "    return filtre\n",
    "\n",
    "def ensemble_de_utilisateurs_mentions_hashtag(df, hashtag):\n",
    "    \"\"\"\n",
    "    Retourne le nombre de publications par topics.\n",
    "    \"\"\"\n",
    "    filtre = df[df['hashtag'] == hashtag]\n",
    "    return filtre\n",
    "    \n",
    "def ensemble_de_utilisateurs_mentionnés_utilisateur(df, utilisateur):\n",
    "    \"\"\"\n",
    "    Retourne les mentions d'un utilisateur.\n",
    "    \"\"\"\n",
    "    tweets_u = df[df['auteur'] == utilisateur]\n",
    "    mentions = tweets_u['mentions'].explode()\n",
    "    return list(mentions)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> Pour améliorer ce code il faudrait utiliser nltk pour la tokenisation et la lemmenisation : ça va couper les tweet en petit mots et reduire les mots en leur forme conique ( je n'arrive pas à utilser nltk pour faire ça, erreur)\n",
    "\n",
    "Ducoup la c pas précis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des résultats\n",
    "\n",
    "\n",
    "def nbr_hashtag(df):\n",
    "    hashtags, counts = publications_par_hashtag(df)\n",
    "    plt.bar(hashtags,counts, color = 'blue')\n",
    "    plt.title(\"Nombre de publication par hashtag\")\n",
    "    plt.xlabel(\"Hashtags\")\n",
    "    plt.ylabel(\"Fréquence\")\n",
    "    plt.show()\n",
    "\n",
    "def nbr_utilisateur(df):\n",
    "    utilisateur, counts = publications_par_utilisateurs(df)\n",
    "    plt.bar(utilisateur,counts, color = 'rouge')\n",
    "    plt.title(\"Nombre de publication par utilisateur\")\n",
    "    plt.xlabel(\"utilisateurs\")\n",
    "    plt.ylabel(\"Fréquence\")\n",
    "    plt.show()\n",
    "\n",
    "def nbr_topic(df):\n",
    "    topic, counts = publications_par_topic(df)\n",
    "    plt.bar(topic,counts, color = 'rouge')\n",
    "    plt.title(\"Nombre de publication par topic\")\n",
    "    plt.xlabel(\"topics\")\n",
    "    plt.ylabel(\"Fréquence\")\n",
    "    plt.show()\n",
    "\n",
    "def top_hashtgs(df):\n",
    "    element,counts = Top_K_element(df, 'hashtags', k=3)\n",
    "    plt.bar(element,counts, color = 'blue')\n",
    "    plt.title(\"Nombre de publication par hashtag\")\n",
    "    plt.xlabel(f\"top {k} hashtags\")\n",
    "    plt.ylabel(\"Fréquence\")\n",
    "    plt.show()\n",
    "\n",
    "def top_topic(df):\n",
    "    element,counts = Top_K_element(df, 'topic', k=3)\n",
    "    plt.bar(element,counts, color = 'blue')\n",
    "    plt.title(\"Nombre de publication par hashtag\")\n",
    "    plt.xlabel('topics')\n",
    "    plt.ylabel(\"Fréquence\")\n",
    "    plt.show()\n",
    "\n",
    "def top_auteur(df):\n",
    "    element,counts = Top_K_element(df, 'auteur', k=3)\n",
    "    plt.bar(element,counts, color = 'blue')\n",
    "    plt.title(\"Top k utilisateur\")\n",
    "    plt.xlabel('Utilisateur')\n",
    "    plt.ylabel(\"Fréquence\")\n",
    "    plt.show()\n",
    "\n",
    "def top_hashtgs(df):\n",
    "    element,counts = Top_K_element(df, 'mentions', k=3)\n",
    "    plt.bar(element,counts, color = 'blue')\n",
    "    plt.title(\"Nombre de publication par hashtag\")\n",
    "    plt.xlabel('mentions')\n",
    "    plt.ylabel(\"Fréquence\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@ericbailly24 @maxigr04del  mes tontons vous avez fait votre part  JO prochain on ira en demi final au moins BRAVO  vous  #SupportriceMazo #domie #CIV\n",
      "neutre\n",
      "['#SupportriceMazo', '#domie', '#CIV']\n",
      "['@ericbailly24', '@maxigr04del']\n"
     ]
    }
   ],
   "source": [
    "t= \"@ericbailly24 @maxigr04del  mes tontons vous avez fait votre part , JO prochain on ira en demi final au moins. BRAVO à vous . #SupportriceMazo #domie #CIV\"\n",
    "\n",
    "\n",
    "def Nettoyage_Tweet(tweet):\n",
    "    \"\"\"Fonction qui supprime les caractères spéciaux d'un tweet\"\"\"\n",
    "    enlever_speciaux = re.sub(r'[^a-zA-Z0-9\\s@#]', '', tweet)\n",
    "    return enlever_speciaux\n",
    "\n",
    "def Extraction_Sentiments(tweet): \n",
    "        \"\"\"Extrait le sentiment du texte\"\"\"\n",
    "        blob = TextBlob(tweet)\n",
    "        polarite =  blob.sentiment.polarity\n",
    "        if polarite > 0:\n",
    "            sentiment = 'positif'\n",
    "        elif polarite < 0:\n",
    "            sentiment = 'négatif'\n",
    "        elif polarite == 0:\n",
    "            sentiment = 'neutre'\n",
    "        return sentiment\n",
    "    \n",
    "def Extraction_Hashtags(tweet):  #A Faire \n",
    "        \"\"\"\"\"\"\n",
    "        hashtags = re.findall(r'#\\w+', tweet)\n",
    "        return hashtags\n",
    "    \n",
    "def Extraction_Mentions(tweet):  #A Faire\n",
    "        \"\"\"Fonction qui extrait les mentions de personnes dans un tweet\"\"\"\n",
    "        mentions = re.findall(r'@[A-Za-z0-9_]+', tweet)\n",
    "        return mentions\n",
    "\n",
    "\n",
    "print(Nettoyage_Tweet(t))\n",
    "print(Extraction_Sentiments(t))\n",
    "print(Extraction_Hashtags(t))\n",
    "print(Extraction_Mentions(t))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le tweet :\n",
      "A balanced diet can prevent many diseases.\n",
      "Appartient au Topic : year, world\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open ('test.json','r',encoding='utf-8') as f:\n",
    "    donne= json.load(f)\n",
    "\n",
    "def Id_Topic(tweet):\n",
    "\n",
    "    def traitement_texte(text):\n",
    "        text= text.lower() # mettre en minuscule\n",
    "        text= re.sub(r'[^a-z\\s]', '', text) \n",
    "        return text\n",
    "    def get_stopwords():\n",
    "        return stopwords.words('english')\n",
    "    stopwords_list = get_stopwords()\n",
    "    tweets= []\n",
    "\n",
    "        \n",
    "    tweets = [infos[\"text\"] for infos in donne]\n",
    "        \n",
    "    texte_traite = [traitement_texte(tweet) for tweet in tweets]\n",
    "        \n",
    "    vecteur = TfidfVectorizer(stop_words= stopwords_list)  # Utilise les stopwords en français intégrés dans sklearn\n",
    "    a = vecteur.fit_transform(texte_traite)\n",
    "\n",
    "    # n_components=2 : Le modèle cherchera à identifier 2 sujets principaux dans les données textuelles.\n",
    "    lda = LatentDirichletAllocation(n_components=2, random_state=42)\n",
    "    lda.fit(a) # on utilise LDA sur la matrice X \n",
    "\n",
    "    topics = []\n",
    "    # Récupération du vocabulaire extrait par TfidfVectorizer\n",
    "    words = vecteur.get_feature_names_out()\n",
    "    for i, topic_weights in enumerate(lda.components_):\n",
    "        top_indices = topic_weights.argsort()[:-3:-1]  # Prendre les 2 mots les plus importants\n",
    "        top_words = [words[i] for i in top_indices]  # Utilisation des indices pour récupérer les mots correspondants dans la liste `words`.\n",
    "        topics.append(f\": {', '.join(top_words)}\")\n",
    "\n",
    "    tweet_traite = traitement_texte(tweet)\n",
    "    vecteur_tweet = vecteur.transform([tweet_traite])\n",
    "    probabilities = lda.transform(vecteur_tweet)\n",
    "    topic_idx = probabilities.argmax()  # Trouve l'indice du topic avec la probabilité la plus élevée\n",
    "\n",
    "    print(f\"Le tweet :\\n{tweet}\\nAppartient au Topic {topics[topic_idx ]}\")\n",
    "\n",
    "tweet = \"A balanced diet can prevent many diseases.\"\n",
    "\n",
    "Id_Topic(tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lnberroug/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le tweet :\n",
      "Intermittent fasting shows great health benefits.\n",
      "Mots principaux du Topic 3 : great, fasting, benefits, shows, intermittent\n",
      "Thème principal estimé : Santé (Basé sur les mots clés des topics)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Télécharger les stopwords si nécessaire\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Définir les thèmes principaux avec leurs mots-clés caractéristiques\n",
    "themes = {\n",
    "    \"Technologie\": [\"smartphone\", \"ai\", \"artificial\", \"intelligence\", \"tesla\", \"5g\", \"autonomous\", \"future\", \"vehicle\", \"robot\", \"machine\", \"algorithm\", \"data\", \"programming\"],\n",
    "    \"Sports\": [\"gold\", \"olympics\", \"champions\", \"league\", \"serena\", \"athletes\", \"football\", \"team\", \"match\", \"qualifiers\", \"score\", \"tournament\", \"basketball\", \"goal\"],\n",
    "    \"Santé\": [\"mental\", \"health\", \"vaccination\", \"fasting\", \"diet\", \"exercise\", \"life\", \"disease\", \"awareness\", \"nutrition\", \"wellness\", \"hospital\", \"doctor\", \"medicine\"],\n",
    "    \"Divertissement\": [\"adele\", \"album\", \"marvel\", \"movie\", \"series\", \"netflix\", \"oscars\", \"swift\", \"tour\", \"boxoffice\", \"music\", \"concert\", \"theater\", \"festival\"],\n",
    "    \"Politique\": [\"election\", \"government\", \"policy\", \"minister\", \"law\", \"debate\", \"vote\", \"campaign\", \"president\", \"parliament\", \"senate\", \"democracy\"],\n",
    "    \"Économie\": [\"market\", \"stock\", \"economy\", \"finance\", \"investment\", \"growth\", \"business\", \"trade\", \"money\", \"tax\", \"employment\", \"entrepreneur\"],\n",
    "    \"Éducation\": [\"school\", \"university\", \"student\", \"teacher\", \"class\", \"learning\", \"exam\", \"homework\", \"course\", \"degree\", \"education\", \"knowledge\", \"scholarship\"],\n",
    "    \"Climat\": [\"climate\", \"global\", \"warming\", \"environment\", \"sustainability\", \"pollution\", \"green\", \"renewable\", \"recycle\", \"weather\", \"carbon\", \"planet\", \"biodiversity\"],\n",
    "    \"Voyages\": [\"travel\", \"trip\", \"vacation\", \"flight\", \"hotel\", \"destination\", \"adventure\", \"explore\", \"tourism\", \"cruise\", \"passport\", \"beach\", \"holiday\"],\n",
    "    \"Cuisine\": [\"recipe\", \"dish\", \"meal\", \"cooking\", \"baking\", \"chef\", \"restaurant\", \"kitchen\", \"ingredients\", \"food\", \"menu\", \"flavor\", \"dinner\", \"lunch\"]\n",
    "}\n",
    "\n",
    "\n",
    "# Charger les données\n",
    "with open('test.json', 'r', encoding='utf-8') as f:\n",
    "    donne = json.load(f)\n",
    "\n",
    "def Id_Topic(tweet):\n",
    "    # Fonction pour nettoyer le texte\n",
    "    def traitement_texte(text):\n",
    "        text = text.lower()  # Convertir en minuscule\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)  # Supprimer les caractères spéciaux\n",
    "        return text\n",
    "\n",
    "    def get_stopwords():\n",
    "        return stopwords.words('french')\n",
    "\n",
    "    stopwords_list = get_stopwords()\n",
    "    tweets = [infos[\"text\"] for infos in donne]\n",
    "\n",
    "    # Nettoyer les textes des tweets\n",
    "    texte_traite = [traitement_texte(tweet) for tweet in tweets]\n",
    "\n",
    "    # Initialiser et entraîner le vecteur TF-IDF\n",
    "    vecteur = TfidfVectorizer(stop_words=stopwords_list)\n",
    "    a = vecteur.fit_transform(texte_traite)\n",
    "\n",
    "    # Entraîner le modèle LDA\n",
    "    lda = LatentDirichletAllocation(n_components=20, random_state=42)\n",
    "    lda.fit(a)\n",
    "\n",
    "    # Récupérer les topics\n",
    "    topics = []\n",
    "    words = vecteur.get_feature_names_out()\n",
    "    for i, topic_weights in enumerate(lda.components_):\n",
    "        top_indices = topic_weights.argsort()[:-6:-1]  # Prendre les 2 mots les plus importants\n",
    "        top_words = [words[i] for i in top_indices]\n",
    "        topics.append(top_words)\n",
    "\n",
    "    # Prédire le topic pour le tweet donné\n",
    "    tweet_traite = traitement_texte(tweet)\n",
    "    vecteur_tweet = vecteur.transform([tweet_traite])\n",
    "    probabilities = lda.transform(vecteur_tweet)\n",
    "    topic_idx = probabilities.argmax()  # Trouver l'indice du topic avec la probabilité la plus élevée\n",
    "    topic_words = topics[topic_idx]  # Mots principaux du topic\n",
    "\n",
    "    # Identifier le thème principal\n",
    "    theme_scores = {}\n",
    "    for theme, keywords in themes.items():\n",
    "        common_words = set(topic_words).intersection(set(keywords))\n",
    "        theme_scores[theme] = len(common_words)\n",
    "\n",
    "    # Ajouter un thème \"Inconnu\" si aucun mot ne correspond\n",
    "    if max(theme_scores.values()) == 0:\n",
    "        closest_theme = \"Inconnu\"\n",
    "    else:\n",
    "        closest_theme = max(theme_scores, key=theme_scores.get)  # Trouver le thème avec le plus grand score\n",
    "\n",
    "    # Résultats\n",
    "    print(f\"Le tweet :\\n{tweet}\")\n",
    "    print(f\"Mots principaux du Topic {topic_idx} : {', '.join(topic_words)}\")\n",
    "    print(f\"Thème principal estimé : {closest_theme} (Basé sur les mots clés des topics)\")\n",
    "\n",
    "# Exemple de tweet\n",
    "tweet = \"Intermittent fasting shows great health benefits.\"\n",
    "Id_Topic(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics identifiés : ['technologie']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Liste de mots-clés pour les topics\n",
    "TOPICS = {\n",
    "    \"technologie\": [\"AI\", \"machine learning\", \"cloud\", \"blockchain\"],\n",
    "    \"environnement\": [\"écologie\", \"biodiversité\", \"climat\", \"pollution\"],\n",
    "    \"santé\": [\"médecine\", \"vaccin\", \"hôpital\", \"maladie\"],\n",
    "    \"politique\": [\"élection\", \"gouvernement\", \"loi\", \"politique\"]\n",
    "}\n",
    "\n",
    "def id_topic(tweet_text):\n",
    "\n",
    "    \"\"\"Identifie les topics d'une publication en fonction de mots-clés.\"\"\"\n",
    "    topics_found = []\n",
    "    \n",
    "    # Nettoyer le texte du tweet\n",
    "    clean_text = re.sub(r\"[^\\w\\s]\", \"\", tweet_text.lower())\n",
    "    \n",
    "    # Parcourir les topics et leurs mots-clés\n",
    "    for topic, keywords in TOPICS.items():\n",
    "        for keyword in keywords:\n",
    "            if keyword.lower() in clean_text:\n",
    "                topics_found.append(topic)\n",
    "                break  # On évite de répéter le même topic\n",
    "    \n",
    "    return topics_found if topics_found else [\"non catégorisé\"]\n",
    "\n",
    "# Exemple d'utilisation\n",
    "tweet = \"I'm at Gardens of Versailles in Versailles\"\n",
    "result = id_topic(tweet)\n",
    "print(\"Topics identifiés :\", result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
