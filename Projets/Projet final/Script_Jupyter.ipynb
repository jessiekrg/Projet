{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etudiantes\n",
    "Lina Berroug\n",
    "et\n",
    "Jessica Karega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTATION DE MODULES\n",
    "\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import gradio as gr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Télécharger les stopwords en français si ce n'est pas déjà fait\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test2.json', 'r', encoding='utf-8') as f:\n",
    "    donne = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Ce notebook analyse les tweets d'un fichier JSON pour extraire des informations et les exploiter grâce à un dataFrame\n",
    "\n",
    "Nous utiliserons les bibliothèques suivantes :\n",
    "- `json` : pour manipuler les fichiers JSON\n",
    "- `pandas` : pour analyser les données tabulaires\n",
    "- `re` : pour extraire ou supprimer des expressions régulière d'un tweet\n",
    "- `TextBlob` : pour l'analyse de sentiments\n",
    "- `matplotlib` : pour visualiser sous forme de graphiques les données\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagramme des étapes de traitement\n",
    "Schéma qui montre les étapes suivies pour le projet :\n",
    "\n",
    "<img src=\"diagramme.png\" alt=\"Description de l'image\" style=\"width:40%; height:auto;\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>**PARTIE I** : Création de la classe Tweet et sa soumission dans la Landing Zone</u>\n",
    "\n",
    "## Création de la classe Tweet\n",
    "-  Nettoyer le texte brut.\n",
    "\n",
    "\n",
    "## Création de la classe ZA\n",
    "- Enrichir et structurer les données nettoyées.\n",
    "\n",
    "    ### 1. Méthode : Extraction_Hashtags \n",
    "    - on utilise une regex pour extraire des hashtags standards \n",
    "    ### 2. Méthode : Extraction_Mentions \n",
    "    - on utilise une regex pour renvoyer toutes les correspondances d'un motif (mentions @) sous forme de liste.\n",
    "    ### 3. Méthode : Extraction_Sentiments\n",
    "    - on utilise Textblob pour déterminer le sentiment d'un texte en renvoyant une polarité (positive, neutre, négative) et une subjectivité.\n",
    "    ### 4. Méthode : Id_Topic\n",
    "\n",
    "\n",
    "## Création Zone d'Atterissage\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARTIE : CREATION DU TWEET + SOUMISSION DU TWEET DANS LANDING ZONE\n",
    "\n",
    "#Création d'une Classe Tweet à laquelle seront associées les attributs suivant : auteurs, hashtag, mentions, sentiments, (topics???)\n",
    "\n",
    "class Tweet: #Plan/Modèle\n",
    "    def __init__(self, contenu): #Méthode (assisstant qui aide à donner à chaque tweet ses attributs)\n",
    "        self.contenu = contenu\n",
    "\n",
    "    def Nettoyage_Tweet(self):\n",
    "        \"\"\"Fonction qui supprime les caractères spéciaux d'un tweet\"\"\"\n",
    "        enlever_speciaux = re.sub(r'[^a-zA-Z0-9\\s@#]', '', self.contenu)\n",
    "        return enlever_speciaux\n",
    "\n",
    "class ZA:\n",
    "    def __init__(self, id, auteur, texte): #Méthode (assisstant qui aide à donner à chaque tweet ses attributs)\n",
    "        self.id = id\n",
    "        self.auteur = auteur\n",
    "        self.text = texte\n",
    "        self.hashtags = self.Extraction_Hashtags()\n",
    "        self.mentions = self.Extraction_Mentions()\n",
    "        self.sentiment = self.Extraction_Sentiments()\n",
    "        self.topic_words = self.Id_Topic()\n",
    "\n",
    "    \n",
    "    def Extraction_Sentiments(self): \n",
    "        \"\"\"Extrait le sentiment du texte\"\"\"\n",
    "        blob = TextBlob(self.text)\n",
    "        polarite =  blob.sentiment.polarity\n",
    "        if polarite > 0:\n",
    "            sentiment = 'positif'\n",
    "        elif polarite < 0:\n",
    "            sentiment = 'négatif'\n",
    "        elif polarite == 0:\n",
    "            sentiment = 'neutre'\n",
    "        return sentiment\n",
    "    \n",
    "    def Extraction_Hashtags(self):  #A Faire \n",
    "        \"\"\"\"\"\"\n",
    "        hashtags = re.findall(r'#\\w+', self.text)\n",
    "        return hashtags\n",
    "    \n",
    "    def Extraction_Mentions(self):  #A Faire\n",
    "        \"\"\"Fonction qui extrait les mentions de personnes dans un tweet\"\"\"\n",
    "        mentions = re.findall(r'@[A-Za-z0-9_]+', self.text)\n",
    "\n",
    "        return mentions\n",
    "\n",
    "\n",
    "    def Id_Topic(self):\n",
    "        \n",
    "        # Fonction pour nettoyer le texte\n",
    "        def traitement_texte(text):\n",
    "            text = text.lower()  # Convertir en minuscule\n",
    "            text = re.sub(r'[^a-z\\s]', '', text)  # Supprimer les caractères spéciaux\n",
    "            return text\n",
    "\n",
    "        def get_stopwords():\n",
    "            return stopwords.words('french')\n",
    "\n",
    "        stopwords_list = get_stopwords()\n",
    "        tweets = [infos[\"text\"] for infos in donne]\n",
    "\n",
    "        # Nettoyer les textes des tweets\n",
    "        texte_traite = [traitement_texte(tweet) for tweet in tweets]\n",
    "\n",
    "        # Initialiser et entraîner le vecteur TF-IDF\n",
    "        vecteur = TfidfVectorizer(stop_words=stopwords_list)\n",
    "        a = vecteur.fit_transform(texte_traite)\n",
    "\n",
    "        # Entraîner le modèle LDA\n",
    "        lda = LatentDirichletAllocation(n_components=20, random_state=42)\n",
    "        lda.fit(a)\n",
    "\n",
    "        # Récupérer les topics\n",
    "        topics = []\n",
    "        words = vecteur.get_feature_names_out()\n",
    "        for i, topic_weights in enumerate(lda.components_):\n",
    "            top_indices = topic_weights.argsort()[:-3:-1]  # Prendre les 2 mots les plus importants\n",
    "            top_words = [words[i] for i in top_indices]\n",
    "            topics.append(top_words)\n",
    "\n",
    "        # Prédire le topic pour le tweet donné\n",
    "        tweet_traite = traitement_texte(self.text)\n",
    "        vecteur_tweet = vecteur.transform([tweet_traite])\n",
    "        probabilities = lda.transform(vecteur_tweet)\n",
    "        topic_idx = probabilities.argmax()  # Trouver l'indice du topic avec la probabilité la plus élevée\n",
    "        topic_words = topics[topic_idx]  # Mots principaux du topic\n",
    "\n",
    "        return topic_words\n",
    "\n",
    "\n",
    "def Zone_Atterisssage (Fichier):\n",
    "        \"\"\"Stockage du tweet dans le fichier JSON zone d'atterissage\"\"\"\n",
    "        ZA_liste=[]\n",
    "\n",
    "        repertoire_courant = os.getcwd()\n",
    "        fichier_entrant = os.path.join(repertoire_courant, Fichier)\n",
    "        fichier_sortant = os.path.join(repertoire_courant, 'outputs', 'ZA.json')\n",
    "        \n",
    "        with open (fichier_entrant,'r',encoding='utf-8') as f:\n",
    "            donnee= json.load(f)\n",
    "            \n",
    "        for infos_tweets in donnee:\n",
    "            id = infos_tweets.get(\"id\")\n",
    "            auteur = infos_tweets.get(\"author_id\")\n",
    "            contenu = infos_tweets.get(\"text\")\n",
    "\n",
    "            tweet = Tweet(contenu)\n",
    "            contenu_nettoye = tweet.Nettoyage_Tweet()\n",
    "    \n",
    "\n",
    "            tweet_analyse = ZA(id, auteur, contenu_nettoye)\n",
    "\n",
    "\n",
    "            dict_tweet = {\n",
    "                \"id\": tweet_analyse.id,\n",
    "                \"auteur\": tweet_analyse.auteur,\n",
    "                \"tweet\": contenu_nettoye,\n",
    "                \"texte\": tweet_analyse.text,\n",
    "                \"hashtags\" : tweet_analyse.hashtags,\n",
    "                \"mentions\" : tweet_analyse.mentions,\n",
    "                \"sentiment\" : tweet_analyse.sentiment,\n",
    "                \"topic\" : tweet_analyse.topic_words\n",
    "            }\n",
    "            ZA_liste.append(dict_tweet)\n",
    "\n",
    "        \n",
    "        os.makedirs(os.path.dirname(fichier_sortant), exist_ok=True)\n",
    "        with open(fichier_sortant,'w', encoding='utf-8') as f:\n",
    "            json.dump(ZA_liste, f, indent=4, ensure_ascii=False)\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    Zone_Atterisssage('test2.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>**PARTIE II** : Data Frame et Visualisation des données</u>\n",
    "\n",
    "## Création de la DF\n",
    "- structure qui organise les données en tables bidimensionnelles de lignes et de colonnes, comparables à une feuille de calcul\n",
    "\n",
    "\n",
    "## Fonction d'analyses avancées\n",
    "        - Top K éléments : \n",
    "        - Publications_par_hashtag : \n",
    "        - Publications_par_utilisateurs : \n",
    "        - publications_par_topic : \n",
    "        - tweets_par_utilisateur : \n",
    "\n",
    "\n",
    "## Fonction de Visualisation sur Maplotlib et Gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Vos fonctions d'analyse\n",
    "def CreerDataFrame(fichier_json):\n",
    "    df  = pd.read_json(fichier_json)\n",
    "    return df\n",
    "\n",
    "def Top_K_element(df, colonne, k=3):\n",
    "    top_k = df[colonne].explode().value_counts().head(k)\n",
    "    element= top_k.index.tolist()\n",
    "    counts = top_k.values.tolist()\n",
    "    return element, counts\n",
    "\n",
    "def publications_par_hashtag(df):\n",
    "    hashtags_explodes = df['hashtags'].explode()\n",
    "    hashtag_counts = hashtags_explodes.value_counts()\n",
    "\n",
    "    hashtags = hashtag_counts.index.tolist()\n",
    "    counts = hashtag_counts.values.tolist()\n",
    "    return hashtags, counts\n",
    "\n",
    "def publications_par_utilisateurs(df):\n",
    "    utilisateur_counts = df['auteur'].value_counts()\n",
    "    utilisateur = utilisateur_counts.index.tolist()  \n",
    "    counts = utilisateur_counts.values.tolist()   \n",
    "    return utilisateur, counts\n",
    "\n",
    "def publications_par_topic(df):\n",
    "    topics_counts = df['topics'].value_counts()\n",
    "    topic = topics_counts.index.tolist()  \n",
    "    counts = topics_counts.values.tolist()   \n",
    "    return topic , counts\n",
    "\n",
    "def tweets_par_utilisateur(df, utilisateur):\n",
    "    filtre = df[df['auteur'] == utilisateur]\n",
    "    tweets = filtre['contenu'].tolist() \n",
    "    utilisateurs = filtre['auteur'].tolist() \n",
    "    return utilisateurs, tweets\n",
    "\n",
    "# Fonctions de visualisation modifiées pour retourner des figures\n",
    "def nbr_hashtag(df):\n",
    "    hashtags, counts = publications_par_hashtag(df)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(hashtags, counts, color = 'blue')\n",
    "    ax.set_title(\"Nombre de publication par hashtag\")\n",
    "    ax.set_xlabel(\"Hashtags\")\n",
    "    ax.set_ylabel(\"Fréquence\")\n",
    "    plt.xticks(rotation=50, ha='right')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def nbr_utilisateur(df):\n",
    "    utilisateur, counts = publications_par_utilisateurs(df)\n",
    "    utilisateur = [str(u) for u in utilisateur]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(utilisateur, counts, color = 'red')\n",
    "    ax.set_title(\"Nombre de publication par utilisateur\")\n",
    "    ax.set_xlabel(\"utilisateurs\")\n",
    "    ax.set_ylabel(\"Fréquence\")\n",
    "    plt.xticks(rotation=50, ha='right')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def nbr_topic(df):\n",
    "    topic, counts = publications_par_topic(df)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(topic, counts, color = 'green')\n",
    "    ax.set_title(\"Nombre de publication par topic\")\n",
    "    ax.set_xlabel(\"topics\")\n",
    "    ax.set_ylabel(\"Fréquence\")\n",
    "    plt.xticks(rotation=50, ha='right')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def top_hashtags(df, k):\n",
    "    element, counts = Top_K_element(df, 'hashtags', k)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(element, counts, color = 'purple')\n",
    "    ax.set_title(\"Top Hashtags\")\n",
    "    ax.set_xlabel(f\"Top {k} hashtags\")\n",
    "    ax.set_ylabel(\"Fréquence\")\n",
    "    plt.xticks(rotation=50, ha='right')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def top_auteur(df, k):\n",
    "    element, counts = Top_K_element(df, 'auteur', k)\n",
    "    element = [str(e) for e in element]\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(element, counts, color='orange')\n",
    "    ax.set_title(f\"Top {k} utilisateurs par nombre de publications\")\n",
    "    ax.set_xlabel(\"Utilisateur\")\n",
    "    ax.set_ylabel(\"Fréquence\")\n",
    "    plt.xticks(rotation=50, ha='right')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def top_mentions(df, k):\n",
    "    element, counts = Top_K_element(df, 'mentions', k)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(element, counts, color = 'blue')\n",
    "    ax.set_title(\"Top mentions\")\n",
    "    ax.set_xlabel('mentions')\n",
    "    ax.set_ylabel(\"Fréquence\")\n",
    "    plt.xticks(rotation=50, ha='right')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def utilisateurs_mentions_hashtag(df, hashtag):\n",
    "    tweets = df[df['hashtags'].apply(lambda hashtags: hashtag in hashtags if isinstance(hashtags, list) else False)]\n",
    "    utilisateurs_count = tweets['auteur'].value_counts()\n",
    "    utilisateurs = [str(u) for u in utilisateurs_count.index]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(utilisateurs, utilisateurs_count.values, color='green')\n",
    "    ax.set_title(f\"Nombre de tweets mentionnant {hashtag}\")\n",
    "    ax.set_xlabel(\"Utilisateurs\")\n",
    "    ax.set_ylabel(\"Nombre de tweets\")\n",
    "    plt.xticks(rotation=50, ha='right')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def utilisateur_mentions_utilisateur(df, mention):\n",
    "    tweets = df[df['mentions'].apply(lambda mentions: mention in mentions if isinstance(mentions,list) else False )]\n",
    "    utilisateurs_count = tweets['auteur'].value_counts()\n",
    "    utilisateurs = [str(u) for u in utilisateurs_count.index]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(utilisateurs, utilisateurs_count.values, color='green')\n",
    "    ax.set_title(f\"Nombre de tweets mentionnant {mention}\")\n",
    "    ax.set_xlabel(\"Utilisateurs\")\n",
    "    ax.set_ylabel(\"Nombre de tweets\")\n",
    "    plt.xticks(rotation=50, ha='right')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def utilisateurs_mentionnes_par_utilisateur(df, utilisateur):\n",
    "    df['auteur'] = df['auteur'].astype(str)\n",
    "    tweets_utilisateur = df[df['auteur'] == utilisateur]\n",
    "\n",
    "    if tweets_utilisateur.empty:\n",
    "        return None\n",
    "\n",
    "    mentions = tweets_utilisateur['mentions'].explode()\n",
    "\n",
    "    if mentions.empty:\n",
    "        return None\n",
    "\n",
    "    mentions_count = mentions.value_counts()\n",
    "    mentions_index = [str(m) for m in mentions_count.index]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(mentions_index, mentions_count.values, color='purple')\n",
    "    ax.set_title(f\"Utilisateurs mentionnés par '{utilisateur}'\")\n",
    "    ax.set_xlabel(\"Utilisateurs mentionnés\")\n",
    "    ax.set_ylabel(\"Nombre de mentions\")\n",
    "    plt.xticks(rotation=50, ha='right')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Fonctions Gradio\n",
    "def charger_df(fichier):\n",
    "    df = CreerDataFrame(fichier.name)\n",
    "    return df\n",
    "\n",
    "def afficher_nbr_utilisateur(file):\n",
    "    df = charger_df(file)\n",
    "    return nbr_utilisateur(df)\n",
    "\n",
    "def afficher_nbr_hashtag(file):\n",
    "    df = charger_df(file)\n",
    "    return nbr_hashtag(df)\n",
    "\n",
    "def afficher_nbr_topic(file):\n",
    "    df = charger_df(file)\n",
    "    return nbr_topic(df)\n",
    "\n",
    "def afficher_top_hashtags(file, k):\n",
    "    df = charger_df(file)\n",
    "    return top_hashtags(df, int(k))\n",
    "\n",
    "def afficher_top_auteur(file, k):\n",
    "    df = charger_df(file)\n",
    "    return top_auteur(df, int(k))\n",
    "\n",
    "def afficher_top_mentions(file, k):\n",
    "    df = charger_df(file)\n",
    "    return top_mentions(df, int(k))\n",
    "\n",
    "def afficher_utilisateurs_mentions_hashtag(file, hashtag):\n",
    "    df = charger_df(file)\n",
    "    return utilisateurs_mentions_hashtag(df, hashtag)\n",
    "\n",
    "def afficher_utilisateur_mentions_utilisateur(file, mention):\n",
    "    df = charger_df(file)\n",
    "    return utilisateur_mentions_utilisateur(df, mention)\n",
    "\n",
    "def afficher_utilisateurs_mentionnes_par_utilisateur(file, utilisateur):\n",
    "    df = charger_df(file)\n",
    "    fig = utilisateurs_mentionnes_par_utilisateur(df, utilisateur)\n",
    "    if fig is None:\n",
    "        return \"Aucune donnée\"\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Interface d'Analyse\")\n",
    "\n",
    "    with gr.Tab(\"Nombre de publications par utilisateur\"):\n",
    "        file_input = gr.File(label=\"Charger ZA.json\")\n",
    "        bouton_utilisateur = gr.Button(\"Afficher\")\n",
    "        out_plot_utilisateur = gr.Plot()\n",
    "        bouton_utilisateur.click(afficher_nbr_utilisateur, inputs=file_input, outputs=out_plot_utilisateur)\n",
    "\n",
    "    with gr.Tab(\"Nombre de publications par hashtag\"):\n",
    "        file_input2 = gr.File(label=\"Charger ZA.json\")\n",
    "        bouton_hashtag = gr.Button(\"Afficher\")\n",
    "        out_plot_hashtag = gr.Plot()\n",
    "        bouton_hashtag.click(afficher_nbr_hashtag, inputs=file_input2, outputs=out_plot_hashtag)\n",
    "\n",
    "    with gr.Tab(\"Nombre de publications par topic\"):\n",
    "        file_input3 = gr.File(label=\"Charger ZA.json\")\n",
    "        bouton_topic = gr.Button(\"Afficher\")\n",
    "        out_plot_topic = gr.Plot()\n",
    "        bouton_topic.click(afficher_nbr_topic, inputs=file_input3, outputs=out_plot_topic)\n",
    "\n",
    "    with gr.Tab(\"Top K Hashtags\"):\n",
    "        file_input4 = gr.File(label=\"Charger ZA.json\")\n",
    "        k_input = gr.Number(label=\"K\", value=3)\n",
    "        bouton_top_hashtags = gr.Button(\"Afficher\")\n",
    "        out_plot_top_hashtags = gr.Plot()\n",
    "        bouton_top_hashtags.click(afficher_top_hashtags, inputs=[file_input4, k_input], outputs=out_plot_top_hashtags)\n",
    "\n",
    "    with gr.Tab(\"Top K Auteurs\"):\n",
    "        file_input5 = gr.File(label=\"Charger ZA.json\")\n",
    "        k_input2 = gr.Number(label=\"K\", value=3)\n",
    "        bouton_top_auteur = gr.Button(\"Afficher\")\n",
    "        out_plot_top_auteur = gr.Plot()\n",
    "        bouton_top_auteur.click(afficher_top_auteur, inputs=[file_input5, k_input2], outputs=out_plot_top_auteur)\n",
    "\n",
    "    with gr.Tab(\"Top K Mentions\"):\n",
    "        file_input6 = gr.File(label=\"Charger ZA.json\")\n",
    "        k_input3 = gr.Number(label=\"K\", value=3)\n",
    "        bouton_top_mentions = gr.Button(\"Afficher\")\n",
    "        out_plot_top_mentions = gr.Plot()\n",
    "        bouton_top_mentions.click(afficher_top_mentions, inputs=[file_input6, k_input3], outputs=out_plot_top_mentions)\n",
    "\n",
    "    with gr.Tab(\"Utilisateurs mentionnant un hashtag\"):\n",
    "        file_input7 = gr.File(label=\"Charger ZA.json\")\n",
    "        hashtag_input = gr.Textbox(label=\"Hashtag (ex: #CIV)\")\n",
    "        bouton_mention_hashtag = gr.Button(\"Afficher\")\n",
    "        out_plot_mention_hashtag = gr.Plot()\n",
    "        bouton_mention_hashtag.click(afficher_utilisateurs_mentions_hashtag, inputs=[file_input7, hashtag_input], outputs=out_plot_mention_hashtag)\n",
    "\n",
    "    with gr.Tab(\"Utilisateurs mentionnant un utilisateur\"):\n",
    "        file_input8 = gr.File(label=\"Charger ZA.json\")\n",
    "        mention_input = gr.Textbox(label=\"Utilisateur mentionné (ex: @leonnajulie)\")\n",
    "        bouton_mention_utilisateur = gr.Button(\"Afficher\")\n",
    "        out_plot_mention_utilisateur = gr.Plot()\n",
    "        bouton_mention_utilisateur.click(afficher_utilisateur_mentions_utilisateur, inputs=[file_input8, mention_input], outputs=out_plot_mention_utilisateur)\n",
    "\n",
    "    with gr.Tab(\"Utilisateurs mentionnés par un utilisateur\"):\n",
    "        file_input9 = gr.File(label=\"Charger ZA.json\")\n",
    "        user_input = gr.Textbox(label=\"Auteur (ex: '372993152')\")\n",
    "        bouton_user_mentions = gr.Button(\"Afficher\")\n",
    "        out_plot_user_mentions = gr.Plot()\n",
    "        bouton_user_mentions.click(afficher_utilisateurs_mentionnes_par_utilisateur, inputs=[file_input9, user_input], outputs=out_plot_user_mentions)\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t= \"@ericbailly24 @maxigr04del  mes tontons vous avez fait votre part , JO prochain on ira en demi final au moins. BRAVO à vous . #SupportriceMazo #domie #CIV\"\n",
    "\n",
    "\n",
    "def Nettoyage_Tweet(tweet):\n",
    "    \"\"\"Fonction qui supprime les caractères spéciaux d'un tweet\"\"\"\n",
    "    enlever_speciaux = re.sub(r'[^a-zA-Z0-9\\s@#]', '', tweet)\n",
    "    return enlever_speciaux\n",
    "\n",
    "def Extraction_Sentiments(tweet): \n",
    "        \"\"\"Extrait le sentiment du texte\"\"\"\n",
    "        blob = TextBlob(tweet)\n",
    "        polarite =  blob.sentiment.polarity\n",
    "        if polarite > 0:\n",
    "            sentiment = 'positif'\n",
    "        elif polarite < 0:\n",
    "            sentiment = 'négatif'\n",
    "        elif polarite == 0:\n",
    "            sentiment = 'neutre'\n",
    "        return sentiment\n",
    "    \n",
    "def Extraction_Hashtags(tweet):  #A Faire \n",
    "        \"\"\"\"\"\"\n",
    "        hashtags = re.findall(r'#\\w+', tweet)\n",
    "        return hashtags\n",
    "    \n",
    "def Extraction_Mentions(tweet):  #A Faire\n",
    "        \"\"\"Fonction qui extrait les mentions de personnes dans un tweet\"\"\"\n",
    "        mentions = re.findall(r'@[A-Za-z0-9_]+', tweet)\n",
    "        return mentions\n",
    "\n",
    "\n",
    "print(Nettoyage_Tweet(t))\n",
    "print(Extraction_Sentiments(t))\n",
    "print(Extraction_Hashtags(t))\n",
    "print(Extraction_Mentions(t))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* Running on public URL: https://0e23db567fdf07e35b.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://0e23db567fdf07e35b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/jesse/miniconda3/lib/python3.11/site-packages/gradio/queueing.py\", line 624, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jesse/miniconda3/lib/python3.11/site-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jesse/miniconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 2015, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jesse/miniconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1562, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jesse/miniconda3/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jesse/miniconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Users/jesse/miniconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 943, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jesse/miniconda3/lib/python3.11/site-packages/gradio/utils.py\", line 865, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/gc/9sbl_gbn0kx3ws4_2rsq75bh0000gn/T/ipykernel_17100/1654384927.py\", line 7, in charger_fichier\n",
      "    df = CreerDataFrame(fichier.name)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/gc/9sbl_gbn0kx3ws4_2rsq75bh0000gn/T/ipykernel_17100/561338795.py\", line 3, in CreerDataFrame\n",
      "    df  = pd.read_json(fichier_json)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jesse/miniconda3/lib/python3.11/site-packages/pandas/io/json/_json.py\", line 815, in read_json\n",
      "    return json_reader.read()\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jesse/miniconda3/lib/python3.11/site-packages/pandas/io/json/_json.py\", line 1025, in read\n",
      "    obj = self._get_object_parser(self.data)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jesse/miniconda3/lib/python3.11/site-packages/pandas/io/json/_json.py\", line 1051, in _get_object_parser\n",
      "    obj = FrameParser(json, **kwargs).parse()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jesse/miniconda3/lib/python3.11/site-packages/pandas/io/json/_json.py\", line 1187, in parse\n",
      "    self._parse()\n",
      "  File \"/Users/jesse/miniconda3/lib/python3.11/site-packages/pandas/io/json/_json.py\", line 1402, in _parse\n",
      "    self.obj = DataFrame(\n",
      "               ^^^^^^^^^^\n",
      "  File \"/Users/jesse/miniconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 778, in __init__\n",
      "    mgr = dict_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jesse/miniconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py\", line 503, in dict_to_mgr\n",
      "    return arrays_to_mgr(arrays, columns, index, dtype=dtype, typ=typ, consolidate=copy)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jesse/miniconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py\", line 114, in arrays_to_mgr\n",
      "    index = _extract_index(arrays)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jesse/miniconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py\", line 680, in _extract_index\n",
      "    raise ValueError(\n",
      "ValueError: Mixing dicts with non-Series may lead to ambiguous ordering.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jesse/miniconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 3805, in get_loc\n",
      "    return self._engine.get_loc(casted_key)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"index.pyx\", line 167, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"index.pyx\", line 196, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7081, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7089, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 'topics'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/jesse/miniconda3/lib/python3.11/site-packages/gradio/queueing.py\", line 624, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jesse/miniconda3/lib/python3.11/site-packages/gradio/route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jesse/miniconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 2015, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jesse/miniconda3/lib/python3.11/site-packages/gradio/blocks.py\", line 1562, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jesse/miniconda3/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jesse/miniconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2441, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/Users/jesse/miniconda3/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 943, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jesse/miniconda3/lib/python3.11/site-packages/gradio/utils.py\", line 865, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/gc/9sbl_gbn0kx3ws4_2rsq75bh0000gn/T/ipykernel_17100/1053153178.py\", line 31, in gr_topics\n",
      "    topics, counts = publications_par_topic(df)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/gc/9sbl_gbn0kx3ws4_2rsq75bh0000gn/T/ipykernel_17100/561338795.py\", line 41, in publications_par_topic\n",
      "    topics_counts = df['topics'].value_counts()\n",
      "                    ~~^^^^^^^^^^\n",
      "  File \"/Users/jesse/miniconda3/lib/python3.11/site-packages/pandas/core/frame.py\", line 4102, in __getitem__\n",
      "    indexer = self.columns.get_loc(key)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/jesse/miniconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py\", line 3812, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 'topics'\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Fonctions pour Gradio\n",
    "def charger_fichier(fichier):\n",
    "    df = CreerDataFrame(fichier.name)\n",
    "    return df.head()  # Afficher les 5 premières lignes\n",
    "\n",
    "def gr_top_k(fichier, colonne, k):\n",
    "        df = CreerDataFrame(fichier.name)\n",
    "        elements, counts = Top_K_element(df, colonne, int(k))\n",
    "        return pd.DataFrame({\"Éléments\": elements, \"Occurrences\": counts})\n",
    "\n",
    "\n",
    "def gr_hashtags(fichier):\n",
    "    \n",
    "    df = CreerDataFrame(fichier.name)\n",
    "    hashtags, counts = publications_par_hashtag(df)\n",
    "    return pd.DataFrame({\"Hashtags\": hashtags, \"Occurrences\": counts})\n",
    "    \n",
    "def gr_utilisateurs(fichier):\n",
    "    \n",
    "    df = CreerDataFrame(fichier.name)\n",
    "    utilisateurs, counts = publications_par_utilisateurs(df)\n",
    "    return pd.DataFrame({\"Utilisateurs\": utilisateurs, \"Occurrences\": counts})\n",
    "\n",
    "def gr_topics(fichier):\n",
    "    \n",
    "    df = CreerDataFrame(fichier.name)\n",
    "    topics, counts = publications_par_topic(df)\n",
    "    return pd.DataFrame({\"Topics\": topics, \"Occurrences\": counts})\n",
    "\n",
    "def generate_top_k_graph(df, colonne, k=3):\n",
    "    try:\n",
    "        # Appeler la fonction Top_K_element pour obtenir les éléments les plus fréquents\n",
    "        elements, counts = Top_K_element(df, colonne, k)\n",
    "        \n",
    "        # Créer un graphique\n",
    "        fig, ax = plt.subplots(figsize=(6, 4))\n",
    "        ax.bar(elements, counts, color='purple')\n",
    "        ax.set_title(f\"Top {k} {colonne}\")\n",
    "        ax.set_xlabel(colonne)\n",
    "        ax.set_ylabel(\"Fréquence\")\n",
    "        ax.tick_params(axis=\"x\", rotation=50)\n",
    "\n",
    "        # Sauvegarder l'image dans un fichier\n",
    "        img_path = os.path.join(graph_sortis, f\"top_{colonne}_{k}.png\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(img_path)  # Sauvegarder l'image dans le fichier\n",
    "        plt.close()  # Fermer pour libérer la mémoire\n",
    "\n",
    "        return img_path  # Retourner le chemin de l'image générée\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "    \n",
    "\n",
    "def gr_tweets(fichier, utilisateur):\n",
    "\n",
    "    df = CreerDataFrame(fichier.name)\n",
    "    utilisateur, tweets = tweets_par_utilisateur(df, utilisateur)\n",
    "    return {\"Utilisateur\": utilisateur, \"Tweets\": tweets}\n",
    "\n",
    "# Interface Gradio\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"Analyse des données JSON avec Gradio\")\n",
    "    \n",
    "    with gr.Tab(\"Charger le fichier\"):\n",
    "        fichier = gr.File(label=\"Télécharger un fichier JSON\")\n",
    "        bouton_charger = gr.Button(\"Charger\")\n",
    "        sortie_df = gr.Dataframe()\n",
    "        bouton_charger.click(charger_fichier, inputs=[fichier], outputs=[sortie_df])\n",
    "\n",
    "    with gr.Tab(\"Top K éléments\"):\n",
    "        fichier_top_k = gr.File(label=\"Télécharger un fichier JSON\")\n",
    "        colonne = gr.Textbox(label=\"Nom de la colonne\", value=\"hashtags\")  # Exemple : hashtags, auteur, etc.\n",
    "        k = gr.Number(label=\"K\", value=3)  # Par défaut, afficher les 3 premiers éléments\n",
    "        bouton_top_k = gr.Button(\"Afficher Top K\")\n",
    "        sortie_top_k_graph = gr.Image(label=\"Graphique Top K\")  # Afficher le graphique ici\n",
    "\n",
    "        # Lier le bouton pour générer le graphique et l'afficher\n",
    "        bouton_top_k.click(generate_top_k_graph, inputs=[fichier_top_k, colonne, k], outputs=[sortie_top_k_graph])\n",
    "\n",
    "\n",
    "    with gr.Tab(\"Publications par hashtags\"):\n",
    "        fichier = gr.File(label=\"Télécharger un fichier JSON\")\n",
    "        bouton_hashtags = gr.Button(\"Afficher les hashtags\")\n",
    "        sortie_hashtags = gr.Dataframe()\n",
    "        bouton_hashtags.click(gr_hashtags, inputs=[fichier], outputs=[sortie_hashtags])\n",
    "\n",
    "    with gr.Tab(\"Publications par utilisateurs\"):\n",
    "        fichier = gr.File(label=\"Télécharger un fichier JSON\")\n",
    "        bouton_utilisateurs = gr.Button(\"Afficher les utilisateurs\")\n",
    "        sortie_utilisateurs = gr.Dataframe()\n",
    "        bouton_utilisateurs.click(gr_utilisateurs, inputs=[fichier], outputs=[sortie_utilisateurs])\n",
    "\n",
    "    with gr.Tab(\"Publications par topics\"):\n",
    "        fichier = gr.File(label=\"Télécharger un fichier JSON\")\n",
    "        bouton_topics = gr.Button(\"Afficher les topics\")\n",
    "        sortie_topics = gr.Dataframe()\n",
    "        bouton_topics.click(gr_topics, inputs=[fichier], outputs=[sortie_topics])\n",
    "\n",
    "    with gr.Tab(\"Tweets par utilisateur\"):\n",
    "        fichier = gr.File(label=\"Télécharger un fichier JSON\")\n",
    "        utilisateur = gr.Textbox(label=\"Nom de l'utilisateur\")\n",
    "        bouton_tweets = gr.Button(\"Afficher les tweets\")\n",
    "        sortie_tweets = gr.JSON()\n",
    "        bouton_tweets.click(gr_tweets, inputs=[fichier, utilisateur], outputs=[sortie_tweets])\n",
    "\n",
    "# Lancer l'application\n",
    "demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
