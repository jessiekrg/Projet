{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTATION DE MODULES\n",
    "#from textblob import TextBlob\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.corpus import stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lnberroug/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lnberroug/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/lnberroug/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Télécharger les stopwords en français si ce n'est pas déjà fait\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (515494132.py, line 26)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[15], line 26\u001b[0;36m\u001b[0m\n\u001b[0;31m    def Extraction_Mentions(self.contenu):  #A Faire\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#PARTIE : CREATION DU TWEET + SOUMISSION DU TWEET DANS LANDING ZONE\n",
    "\n",
    "#Création d'une Classe Tweet à laquelle seront associées les attributs suivant : auteurs, hashtag, mentions, sentiments, (topics???)\n",
    "\n",
    "class Tweet: #Plan/Modèle\n",
    "    def __init__(self, auteur, contenu): #Méthode (assisstant qui aide à donner à chaque tweet ses attributs)\n",
    "        self.auteur = auteur\n",
    "        self.contenu = contenu\n",
    "        self.hashtags = self.Extraction_Hashtags()\n",
    "        self.mentions = self.Extraction_Mentions()\n",
    "        self.sentiment = self.Extraction_Sentiments()\n",
    "\n",
    "    def Nettoyage_Tweet(self):\n",
    "        \"\"\"Fonction qui supprime les caractères spéciaux d'un tweet\"\"\"\n",
    "        enlever_speciaux = re.sub(r'[^a-zA-Z0-9\\s@#]', '', self.contenu)\n",
    "        return enlever_speciaux\n",
    "    \n",
    "    def Zone_Atterisssage (Fichier): # incomplet\n",
    "        \"\"\"Stockage du tweet dans le fichier json zone d'atterissage\"\"\"\n",
    "        \n",
    "        ZA=[]\n",
    "        with open (Fichier,'r',encoding='utf-8') as f:\n",
    "            donnee= json.load(f)\n",
    "            \n",
    "        for infos_tweets in donnee:\n",
    "            auteur = infos_tweets.get(\"author_id\")\n",
    "            contenu = infos_tweets.get(\"text\")\n",
    "            tweet_auteur = {\n",
    "                \"id\": auteur,\n",
    "                \"tweet\": Nettoyage_Tweet(contenu)\n",
    "            }\n",
    "            ZA.append(tweet_auteur)\n",
    "        \n",
    "        with open('ZA.json','w', encoding='utf-8') as f:\n",
    "            json.dump(ZA, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "\n",
    "    \n",
    "    def Extraction_Sentiments(self): \n",
    "        \"\"\"Extrait le sentiment du texte\"\"\"\n",
    "        blob = TextBlob(self.contenu)\n",
    "        polarite =  blob.sentiment.polarity\n",
    "        if polarite > 0:\n",
    "            sentiment = 'positif'\n",
    "        elif polarite < 0:\n",
    "            sentiment = 'négatif'\n",
    "        elif polarite == 0:\n",
    "            sentiment = 'neutre'\n",
    "        return sentiment\n",
    "    \n",
    "    \n",
    "    def Extraction_Hashtags():  #A Faire \n",
    "        \"\"\"\"\"\"\n",
    "        hashtags = re.findall(r'#\\w+', tweet)\n",
    "        return hashtags\n",
    "    \n",
    "    def Extraction_Mentions(self):  #A Faire\n",
    "        \"\"\"Fonction qui extrait les mentions de personnes dans un tweet\"\"\"\n",
    "        mentions = re.findall(r'@[A-Za-z0-9_]+', self.contenu)\n",
    "\n",
    "        return mentions\n",
    "\n",
    "\n",
    "    def Id_Topic(tweet):\n",
    "\n",
    "        def traitement_texte(text):\n",
    "            text= text.lower() # mettre en minuscule\n",
    "            text= re.sub(r'[^a-z\\s]', '', text) \n",
    "            return text\n",
    "        tweets= []\n",
    "        with open ('ZA.json','r',encoding='utf-8') as f:\n",
    "            donnee= json.load(f)\n",
    "        \n",
    "        tweets = [infos[\"tweet\"] for infos in donnee]\n",
    "        \n",
    "        texte_traite = [traitement_texte(tweet) for tweet in tweets]\n",
    "        \n",
    "        vecteur = TfidfVectorizer(stop_words= stopwords.words('french'))  # Utilise les stopwords en français intégrés dans sklearn\n",
    "        X = vecteur.fit_transform(texte_traite)\n",
    "\n",
    "        lda = LatentDirichletAllocation(n_components=2, random_state=42)\n",
    "        lda.fit(X)\n",
    "\n",
    "        topics = []\n",
    "        words = vecteur.get_feature_names_out()\n",
    "        for i, topic_weights in enumerate(lda.components_):\n",
    "            top_indices = topic_weights.argsort()[:-3:-1]  # Prendre les 2 mots les plus importants\n",
    "            top_words = [words[i] for i in top_indices]\n",
    "            topics.append(f\": {', '.join(top_words)}\")\n",
    "\n",
    "\n",
    "        tweet_traite = traitement_texte(tweet)\n",
    "        vecteur = TfidfVectorizer(stop_words='french')  # Utilise les stopwords en français intégrés dans sklearn\n",
    "        vecteur_tweet= vecteur.transform([tweet_traite])\n",
    "        probabilities = lda.transform(vecteur_tweet)\n",
    "        topic = probabilities.argmax()  # Trouve l'indice du topic avec la probabilité la plus élevée\n",
    "\n",
    "        print(f\"Le tweet :\\n{tweet}\\nAppartient au Topic {topics[topic]}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "#tweet = Tweet (auteur, contenu) # Utilisation de la class Tweet\n",
    "        \n",
    "\n",
    "# Process chaque tweet:\n",
    "                \n",
    "with open ('ZA.json','r') as file:\n",
    "    tweets = json.load(file)\n",
    "\n",
    "\n",
    "for tweet in tweets:\n",
    "    print(f\"Auteur : {tweet.auteur}\")\n",
    "    print(f\"Contenu : {tweet.contenu}\")\n",
    "    print(f\"Hashtags : {tweet.hashtags}\")\n",
    "    print(f\"Mentions : {tweet.mentions}\")\n",
    "    print(f\"Sentiment : {tweet.sentiment}\")\n",
    "\n",
    "# --> il faut charger ces données dans une date frame : df = pd.DataFrame(tweets)\n",
    "\n",
    "# yo\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trucs à revoir\n",
    "\n",
    "On utilise la class avec le json Versailles_tweets alors que tous les traitements sont fait à partir de la zone d'atterissage. Il faut changer ça, peut être créer une autre class ? ou juste utilser le tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": fte\n",
      ": entreprises\n",
      ": annonc\n",
      "Le tweet :Le ministre des Sports a proposé une nouvelle loi pour améliorer les infrastructures sportives en France.\n",
      "Appartient au Topic : entreprises\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "ZA = [\n",
    "    {\"tweet\": \"Le gouvernement a annoncé un plan pour soutenir les athlètes locaux en vue des prochains Jeux Olympiques.\"},\n",
    "    {\"tweet\": \"Les débats sur la politique fiscale affectent directement le financement des clubs sportifs nationaux.\"},\n",
    "    {\"tweet\": \"Les politiques sportives du gouvernement sont critiquées par certains partis de l'opposition.\"},\n",
    "    {\"tweet\": \"Les joueurs de football et les politiques de sécurité sont au centre des discussions cette semaine.\"},\n",
    "    {\"tweet\": \"Le ministre des Sports a proposé une nouvelle loi pour améliorer les infrastructures sportives en France.\"},\n",
    "    {\"tweet\": \"L'impact des réformes économiques pourrait affecter la préparation des athlètes pour les compétitions internationales.\"},\n",
    "    {\"tweet\": \"Les équipes de football sont en plein préparatifs pour la Coupe du Monde, malgré les réformes sociales en cours.\"},\n",
    "    {\"tweet\": \"Les débats autour des droits des travailleurs sportifs sont au cœur des discussions législatives.\"},\n",
    "    {\"tweet\": \"Le président a annoncé une série de mesures pour encourager la pratique du sport à tous les niveaux.\"},\n",
    "    {\"tweet\": \"Les discussions sur la sécurité nationale influencent les événements sportifs internationaux, comme la Coupe du Monde.\"},\n",
    "    {\"tweet\": \"Les athlètes se battent pour leurs droits, tout comme les députés débattent des nouvelles réformes sociales.\"},\n",
    "    {\"tweet\": \"La politique de financement des événements sportifs nationaux est une priorité pour le gouvernement.\"},\n",
    "    {\"tweet\": \"Les partisans du football et de la réforme politique se sont réunis pour discuter des changements attendus.\"},\n",
    "    {\"tweet\": \"Le gouvernement tente de promouvoir les sports tout en faisant face à des crises économiques et sociales.\"},\n",
    "    {\"tweet\": \"Les réformes fiscales proposées par le gouvernement pourraient affecter les subventions accordées aux clubs de football.\"},\n",
    "    {\"tweet\": \"Les athlètes sont appelés à soutenir la réforme du système de santé qui impacte leur bien-être.\"},\n",
    "    {\"tweet\": \"Le match de football est plus qu'un simple jeu, c'est aussi un moyen pour le gouvernement de promouvoir ses politiques.\"},\n",
    "    {\"tweet\": \"Les leaders politiques de différents pays se sont réunis pour discuter de la gestion des événements sportifs internationaux.\"},\n",
    "    {\"tweet\": \"Les athlètes soutiennent de plus en plus les initiatives politiques visant à améliorer les conditions de vie des jeunes.\"},\n",
    "    {\"tweet\": \"Les réformes politiques en matière d'urbanisme influencent la construction de nouveaux stades pour les compétitions sportives.\"},\n",
    "    {\"tweet\": \"La diplomatie sportive est devenue un outil pour apaiser les tensions politiques entre les nations.\"},\n",
    "    {\"tweet\": \"Les athlètes de haut niveau se mobilisent pour des changements dans la politique de santé publique.\"},\n",
    "    {\"tweet\": \"La politique de droits de l'homme influence le choix des pays hôtes pour les événements sportifs mondiaux.\"},\n",
    "    {\"tweet\": \"La gestion des crises politiques peut avoir un impact direct sur l'organisation des événements sportifs internationaux.\"},\n",
    "    {\"tweet\": \"Les sportifs ont exprimé leur soutien à la réforme de la loi sur l'immigration, ce qui a créé un débat politique.\"}\n",
    "]\n",
    "\n",
    "def Id_Topic(tweet):\n",
    "\n",
    "        def traitement_texte(text):\n",
    "            text= text.lower() # mettre en minuscule\n",
    "            text= re.sub(r'[^a-z\\s]', '', text)\n",
    "            text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "            return text\n",
    "        def get_stopwords():\n",
    "            return stopwords.words('french')\n",
    "        stopwords_list = get_stopwords()\n",
    "        tweets= []\n",
    "        with open ('ZA.json','r',encoding='utf-8') as f:\n",
    "            donnee= json.load(f)\n",
    "        \n",
    "        tweets = [infos[\"tweet\"] for infos in donnee]\n",
    "        \n",
    "        texte_traite = [traitement_texte(tweet) for tweet in tweets]\n",
    "        \n",
    "        vecteur = TfidfVectorizer(stop_words= stopwords_list)  # Utilise les stopwords en français intégrés dans sklearn\n",
    "        X = vecteur.fit_transform(texte_traite)\n",
    "\n",
    "        lda = LatentDirichletAllocation(n_components=3, random_state=100)\n",
    "        lda.fit(X)\n",
    "\n",
    "\n",
    "        topics = []\n",
    "        words = vecteur.get_feature_names_out()\n",
    "        for i, topic_weights in enumerate(lda.components_):\n",
    "            top_indices = topic_weights.argsort()[:-2:-1]  # Prendre les 2 mots les plus importants\n",
    "            top_words = [words[i] for i in top_indices]\n",
    "            topics.append(f\": {', '.join(top_words)}\")\n",
    "\n",
    "\n",
    "        tweet_traite = traitement_texte(tweet)\n",
    "        vecteur_tweet= vecteur.transform([tweet_traite])\n",
    "        probabilities = lda.transform(vecteur_tweet)\n",
    "        topic = probabilities.argmax()  # Trouve l'indice du topic avec la probabilité la plus élevée\n",
    "\n",
    "        for to in topics:\n",
    "            print (to)\n",
    "\n",
    "        print(f\"Le tweet :{tweet}\\nAppartient au Topic {topics[topic]}\")\n",
    "\n",
    "\n",
    "tweet = \"Le ministre des Sports a proposé une nouvelle loi pour améliorer les infrastructures sportives en France.\"\n",
    "Id_Topic(tweet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> Pour améliorer ce code il faudrait utiliser nltk pour la tokenisation et la lemmenisation : ça va couper les tweet en petit mots et reduire les mots en leur forme conique ( je n'arrive pas à utilser nltk pour faire ça, erreur)\n",
    "\n",
    "Ducoup la c pas précis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
