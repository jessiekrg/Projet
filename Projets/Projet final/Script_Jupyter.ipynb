{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTATION DE MODULES\n",
    "#from textblob import TextBlob\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/jesse/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/jesse/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/jesse/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TÃ©lÃ©charger les stopwords en franÃ§ais si ce n'est pas dÃ©jÃ  fait\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARTIE : CREATION DU TWEET + SOUMISSION DU TWEET DANS LANDING ZONE\n",
    "\n",
    "#CrÃ©ation d'une Classe Tweet Ã  laquelle seront associÃ©es les attributs suivant : auteurs, hashtag, mentions, sentiments, (topics???)\n",
    "\n",
    "class Tweet: #Plan/ModÃ¨le\n",
    "    def __init__(self, contenu): #MÃ©thode (assisstant qui aide Ã  donner Ã  chaque tweet ses attributs)\n",
    "        self.contenu = contenu\n",
    "\n",
    "    def Nettoyage_Tweet(self):\n",
    "        \"\"\"Fonction qui supprime les caractÃ¨res spÃ©ciaux d'un tweet\"\"\"\n",
    "        enlever_speciaux = re.sub(r'[^a-zA-Z0-9\\s@#]', '', self.contenu)\n",
    "        return enlever_speciaux\n",
    "\n",
    "def Zone_Atterisssage (Fichier): # incomplet\n",
    "        \"\"\"Stockage du tweet dans le fichier json zone d'atterissage\"\"\"\n",
    "        ZA=[]\n",
    "        \n",
    "        with open (Fichier,'r',encoding='utf-8') as f:\n",
    "            donnee= json.load(f)\n",
    "            \n",
    "        for infos_tweets in donnee:\n",
    "            auteur = infos_tweets.get(\"author_id\")\n",
    "            contenu = infos_tweets.get(\"text\")\n",
    "            classe = Tweet(contenu)\n",
    "            tweet_auteur = {\n",
    "                \"id\": auteur,\n",
    "                \"tweet\": classe.Nettoyage_Tweet()\n",
    "            }\n",
    "            ZA.append(tweet_auteur)\n",
    "        \n",
    "        df = pd.DataFrame(tweets_traites)\n",
    "        \n",
    "        with open('ZA.json','w', encoding='utf-8') as f:\n",
    "            json.dump(ZA, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "Zone_Atterisssage('versailles_tweets_100.json')\n",
    "\n",
    "class ZA:\n",
    "    def __init__(self,id , texte,chemin_fichier): #MÃ©thode (assisstant qui aide Ã  donner Ã  chaque tweet ses attributs)\n",
    "        self.id = id\n",
    "        self.text = texte\n",
    "        self.chemin_fichier=chemin_fichier\n",
    "        self.hashtags = self.Extraction_Hashtags()\n",
    "        self.mentions = self.Extraction_Mentions()\n",
    "        self.sentiment = self.Extraction_Sentiments()\n",
    "\n",
    "    \n",
    "    def Extraction_Sentiments(self): \n",
    "        \"\"\"Extrait le sentiment du texte\"\"\"\n",
    "        blob = TextBlob(self.text)\n",
    "        polarite =  blob.sentiment.polarity\n",
    "        if polarite > 0:\n",
    "            sentiment = 'positif'\n",
    "        elif polarite < 0:\n",
    "            sentiment = 'nÃ©gatif'\n",
    "        elif polarite == 0:\n",
    "            sentiment = 'neutre'\n",
    "        return sentiment\n",
    "    \n",
    "    def Extraction_Hashtags(self):  #A Faire \n",
    "        \"\"\"\"\"\"\n",
    "        hashtags = re.findall(r'#\\w+', self.text)\n",
    "        return hashtags\n",
    "    \n",
    "    def Extraction_Mentions(self):  #A Faire\n",
    "        \"\"\"Fonction qui extrait les mentions de personnes dans un tweet\"\"\"\n",
    "        mentions = re.findall(r'@[A-Za-z0-9_]+', self.text)\n",
    "\n",
    "        return mentions\n",
    "\n",
    "\n",
    "    def Id_Topic(tweet):\n",
    "\n",
    "        def traitement_texte(text):\n",
    "            text= text.lower() # mettre en minuscule\n",
    "            text= re.sub(r'[^a-z\\s]', '', text) \n",
    "            return text\n",
    "        def get_stopwords():\n",
    "            return stopwords.words('french')\n",
    "        stopwords_list = get_stopwords()\n",
    "        tweets= []\n",
    "        with open ('ZA.json','r',encoding='utf-8') as f:\n",
    "            donnee= json.load(f)\n",
    "        \n",
    "        tweets = [infos[\"tweet\"] for infos in donnee]\n",
    "        \n",
    "        texte_traite = [traitement_texte(tweet) for tweet in tweets]\n",
    "        \n",
    "        vecteur = TfidfVectorizer(stop_words= stopwords_list)  # Utilise les stopwords en franÃ§ais intÃ©grÃ©s dans sklearn\n",
    "        a = vecteur.fit_transform(texte_traite)\n",
    "\n",
    "        # n_components=2 : Le modÃ¨le cherchera Ã  identifier 2 sujets principaux dans les donnÃ©es textuelles.\n",
    "        lda = LatentDirichletAllocation(n_components=2, random_state=42)\n",
    "        lda.fit(a) # on utilise LDA sur la matrice X \n",
    "\n",
    "        topics = []\n",
    "        # RÃ©cupÃ©ration du vocabulaire extrait par TfidfVectorizer\n",
    "        words = vecteur.get_feature_names_out()\n",
    "        for i, topic_weights in enumerate(lda.components_):\n",
    "            top_indices = topic_weights.argsort()[:-3:-1]  # Prendre les 2 mots les plus importants\n",
    "            top_words = [words[i] for i in top_indices]  # Utilisation des indices pour rÃ©cupÃ©rer les mots correspondants dans la liste `words`.\n",
    "\n",
    "            topics.append(f\": {', '.join(top_words)}\")\n",
    "\n",
    "        tweet_traite = traitement_texte(tweet)\n",
    "        vecteur = TfidfVectorizer(stop_words= stopwords_list) # Utilise les stopwords en franÃ§ais intÃ©grÃ©s dans sklearn\n",
    "        vecteur_tweet= vecteur.transform([tweet_traite]) \n",
    "        probabilities = lda.transform(vecteur_tweet)\n",
    "        topic = probabilities.argmax()  # Trouve l'indice du topic avec la probabilitÃ© la plus Ã©levÃ©e\n",
    "\n",
    "        print(f\"Le tweet :\\n{tweet}\\nAppartient au Topic {topics[topic]}\")\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# --> il faut charger ces donnÃ©es dans une date frame : df = pd.DataFrame(tweets)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    id               auteur  \\\n",
      "0  1421616335700824064  1339914264522461187   \n",
      "1  1421599703116943360  1339914264522461187   \n",
      "2  1421599163561742339  1339914264522461187   \n",
      "3  1421591889095057416  1339914264522461187   \n",
      "4  1421582795294617605            717025418   \n",
      "\n",
      "                                             contenu  \\\n",
      "0  Goumin des Ã©lÃ©phants joueurs la mÃªme fatigue m...   \n",
      "1  @ericbailly24 @maxigr04del  mes tontons vous a...   \n",
      "2  Ah oui le sommeil lÃ  sera compliquÃ©. #CIV  est...   \n",
      "3  31 juillet , journÃ©e internationale de la femm...   \n",
      "4           Le pedigree ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ https://t.co/D3Rv7A2BOF   \n",
      "\n",
      "                                     contenu_nettoye  \\\n",
      "0  Goumin des lphants joueurs la mme fatigue mme ...   \n",
      "1  @ericbailly24 @maxigr04del  mes tontons vous a...   \n",
      "2  Ah oui le sommeil l sera compliqu #CIV  est li...   \n",
      "3  31 juillet  journe internationale de la femme ...   \n",
      "4                    Le pedigree  httpstcoD3Rv7A2BOF   \n",
      "\n",
      "                           hashtags                       mentions sentiment  \n",
      "0                     [#twitter225]                             []    neutre  \n",
      "1  [#SupportriceMazo, #domie, #CIV]  [@ericbailly24, @maxigr04del]    neutre  \n",
      "2                            [#CIV]                             []    neutre  \n",
      "3                           [#jifa]                             []    neutre  \n",
      "4                                []                             []    neutre  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class Tweet:\n",
    "    def __init__(self, id, auteur, contenu):\n",
    "        self.id = id\n",
    "        self.auteur = auteur\n",
    "        self.contenu = contenu\n",
    "        self.contenu_nettoye = self.Nettoyage_Tweet()\n",
    "        self.hashtags = self.Extraction_Hashtags()\n",
    "        self.mentions = self.Extraction_Mentions()\n",
    "        self.sentiment = self.Extraction_Sentiments()\n",
    "\n",
    "    def Nettoyage_Tweet(self):\n",
    "        \"\"\"Supprime les caractÃ¨res spÃ©ciaux d'un tweet\"\"\"\n",
    "        return re.sub(r'[^a-zA-Z0-9\\s@#]', '', self.contenu)\n",
    "\n",
    "    def Extraction_Hashtags(self):\n",
    "        return re.findall(r'#\\w+', self.contenu)\n",
    "\n",
    "    def Extraction_Mentions(self):\n",
    "        return re.findall(r'@[A-Za-z0-9_]+', self.contenu)\n",
    "\n",
    "    def Extraction_Sentiments(self):\n",
    "        blob = TextBlob(self.contenu)\n",
    "        polarite = blob.sentiment.polarity\n",
    "        if polarite > 0:\n",
    "            return 'positif'\n",
    "        elif polarite < 0:\n",
    "            return 'nÃ©gatif'\n",
    "        else:\n",
    "            return 'neutre'\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Convertit l'objet Tweet en dictionnaire.\"\"\"\n",
    "        return {\n",
    "            'id': self.id,\n",
    "            'auteur': self.auteur,\n",
    "            'contenu': self.contenu,\n",
    "            'contenu_nettoye': self.contenu_nettoye,\n",
    "            'hashtags': self.hashtags,\n",
    "            'mentions': self.mentions,\n",
    "            'sentiment': self.sentiment\n",
    "        }\n",
    "\n",
    "class ZA:\n",
    "    def __init__(self, chemin_fichier):\n",
    "        self.chemin_fichier = chemin_fichier\n",
    "        self.tweets_data = []  \n",
    "\n",
    "    def charger_tweets(self):\n",
    "        \"\"\"Charge les tweets depuis le fichier JSON et extrait les informations.\"\"\"\n",
    "        with open(self.chemin_fichier, 'r', encoding='utf-8') as f:\n",
    "            donnee = json.load(f)\n",
    "        \n",
    "        for infos_tweets in donnee:\n",
    "            tweet = Tweet(\n",
    "                infos_tweets.get(\"id\"),\n",
    "                infos_tweets.get(\"author_id\"),\n",
    "                infos_tweets.get(\"text\")\n",
    "            )\n",
    "            # Ajoute le dictionnaire du tweet Ã  la liste\n",
    "            self.tweets_data.append(tweet.to_dict())\n",
    "        \n",
    "        # Convertir en DataFrame pour une utilisation ultÃ©rieure\n",
    "        df = pd.DataFrame(self.tweets_data)\n",
    "        \n",
    "        # Sauvegarder dans un fichier JSON si nÃ©cessaire\n",
    "        with open('ZA.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.tweets_data, f, indent=4, ensure_ascii=False)\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Exemple d'utilisation de la classe ZA\n",
    "za = ZA('versailles_tweets_100.json')\n",
    "df_tweets = za.charger_tweets()\n",
    "print(df_tweets.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> Pour amÃ©liorer ce code il faudrait utiliser nltk pour la tokenisation et la lemmenisation : Ã§a va couper les tweet en petit mots et reduire les mots en leur forme conique ( je n'arrive pas Ã  utilser nltk pour faire Ã§a, erreur)\n",
    "\n",
    "Ducoup la c pas prÃ©cis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@ericbailly24 @maxigr04del  mes tontons vous avez fait votre part  JO prochain on ira en demi final au moins BRAVO  vous  #SupportriceMazo #domie #CIV\n",
      "neutre\n",
      "['#SupportriceMazo', '#domie', '#CIV']\n",
      "['@ericbailly24', '@maxigr04del']\n"
     ]
    }
   ],
   "source": [
    "t= \"@ericbailly24 @maxigr04del  mes tontons vous avez fait votre part , JO prochain on ira en demi final au moins. BRAVO Ã  vous . #SupportriceMazo #domie #CIV\"\n",
    "\n",
    "\n",
    "def Nettoyage_Tweet(tweet):\n",
    "    \"\"\"Fonction qui supprime les caractÃ¨res spÃ©ciaux d'un tweet\"\"\"\n",
    "    enlever_speciaux = re.sub(r'[^a-zA-Z0-9\\s@#]', '', tweet)\n",
    "    return enlever_speciaux\n",
    "\n",
    "def Extraction_Sentiments(tweet): \n",
    "        \"\"\"Extrait le sentiment du texte\"\"\"\n",
    "        blob = TextBlob(tweet)\n",
    "        polarite =  blob.sentiment.polarity\n",
    "        if polarite > 0:\n",
    "            sentiment = 'positif'\n",
    "        elif polarite < 0:\n",
    "            sentiment = 'nÃ©gatif'\n",
    "        elif polarite == 0:\n",
    "            sentiment = 'neutre'\n",
    "        return sentiment\n",
    "    \n",
    "def Extraction_Hashtags(tweet):  #A Faire \n",
    "        \"\"\"\"\"\"\n",
    "        hashtags = re.findall(r'#\\w+', tweet)\n",
    "        return hashtags\n",
    "    \n",
    "def Extraction_Mentions(tweet):  #A Faire\n",
    "        \"\"\"Fonction qui extrait les mentions de personnes dans un tweet\"\"\"\n",
    "        mentions = re.findall(r'@[A-Za-z0-9_]+', tweet)\n",
    "        return mentions\n",
    "\n",
    "\n",
    "print(Nettoyage_Tweet(t))\n",
    "print(Extraction_Sentiments(t))\n",
    "print(Extraction_Hashtags(t))\n",
    "print(Extraction_Mentions(t))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le tweet :\n",
      "Le rÃ©chauffement climatique menace la biodiversitÃ©.\n",
      "Appartient au Topic : biodiversit, rchauffement\n"
     ]
    }
   ],
   "source": [
    "donne= [\n",
    "    {\"tweet\": \"Le rÃ©chauffement climatique menace la biodiversitÃ©.\"},\n",
    "    {\"tweet\": \"La pollution des ocÃ©ans est un problÃ¨me mondial.\"},\n",
    "    {\"tweet\": \"Les voitures Ã©lectriques sont une solution Ã©cologique.\"},\n",
    "    {\"tweet\": \"Les panneaux solaires deviennent plus efficaces chaque annÃ©e.\"},\n",
    "    {\"tweet\": \"La dÃ©forestation accÃ©lÃ¨re le changement climatique.\"}\n",
    "]\n",
    "\n",
    "\n",
    "def Id_Topic(tweet):\n",
    "\n",
    "    def traitement_texte(text):\n",
    "        text= text.lower() # mettre en minuscule\n",
    "        text= re.sub(r'[^a-z\\s]', '', text) \n",
    "        return text\n",
    "    def get_stopwords():\n",
    "        return stopwords.words('french')\n",
    "    stopwords_list = get_stopwords()\n",
    "    tweets= []\n",
    "    \n",
    "        \n",
    "    tweets = [infos[\"tweet\"] for infos in donne]\n",
    "        \n",
    "    texte_traite = [traitement_texte(tweet) for tweet in tweets]\n",
    "        \n",
    "    vecteur = TfidfVectorizer(stop_words= stopwords_list)  # Utilise les stopwords en franÃ§ais intÃ©grÃ©s dans sklearn\n",
    "    a = vecteur.fit_transform(texte_traite)\n",
    "\n",
    "    # n_components=2 : Le modÃ¨le cherchera Ã  identifier 2 sujets principaux dans les donnÃ©es textuelles.\n",
    "    lda = LatentDirichletAllocation(n_components=2, random_state=42)\n",
    "    lda.fit(a) # on utilise LDA sur la matrice X \n",
    "\n",
    "    topics = []\n",
    "    # RÃ©cupÃ©ration du vocabulaire extrait par TfidfVectorizer\n",
    "    words = vecteur.get_feature_names_out()\n",
    "    for i, topic_weights in enumerate(lda.components_):\n",
    "        top_indices = topic_weights.argsort()[:-3:-1]  # Prendre les 2 mots les plus importants\n",
    "        top_words = [words[i] for i in top_indices]  # Utilisation des indices pour rÃ©cupÃ©rer les mots correspondants dans la liste `words`.\n",
    "\n",
    "        topics.append(f\": {', '.join(top_words)}\")\n",
    "\n",
    "    tweet_traite = traitement_texte(tweet)\n",
    "    vecteur_tweet= vecteur.transform([tweet_traite]) \n",
    "    probabilities = lda.transform(vecteur_tweet)\n",
    "    topic = probabilities.argmax()  # Trouve l'indice du topic avec la probabilitÃ© la plus Ã©levÃ©e\n",
    "\n",
    "    print(f\"Le tweet :\\n{tweet}\\nAppartient au Topic {topics[topic]}\")\n",
    "\n",
    "tweet = \"Le rÃ©chauffement climatique menace la biodiversitÃ©.\"\n",
    "Id_Topic(tweet)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
